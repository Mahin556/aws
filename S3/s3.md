### References:-
- https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-page-redirect.html
- https://aws.amazon.com/s3/storage-classes/

---

* S3 is an object storage service.
* Storage is virtually infinite.
* It is a core AWS service offering high durability and availability.
* Used for storing files uploaded by users, assets for stateless applications, streamed video files, Terraform state files, and build artifacts with versioning enabled.
* Supports storage + backup use cases.
* Uses a tiered storage system where cost varies based on access frequency (less access = lower cost).
* Can host static websites (no dynamic content, no APIs).
* Supports big data analytics by integrating with services like Athena, EMR, and Redshift.
* Pricing is based on:
  * Data stored
  * Number of requests
  * Data downloaded (data transfer out)
* ‚ÄúFolder‚Äù is a logical concept‚Äîactual container is a **bucket**.
  * Buckets are created in a specific region.
  * Bucket names must be globally unique.
* AMIs and EBS snapshots are stored in S3 internally (managed by AWS).
* A file in S3 is called an **object**.

<br>

* **Bucket naming**

    * Name must be globally unique (no two buckets in the world can have the same name).
    * Length must be **3 to 63 characters**.
    * Only lowercase letters, numbers, hyphens (`-`).
    * No uppercase letters allowed.
    * No spaces or underscores (`_`).
    * Must start with a **lowercase letter or number**.
    * Must end with a **lowercase letter or number** (cannot end with a hyphen).
    * Cannot look like an IP address (e.g., `192.168.0.1` is not allowed).
    * Should avoid using dots (`.`) when hosting static websites with HTTPS because it breaks SSL wildcard matching.
    * Should be DNS-compliant (no special symbols).
    * Should not use "aws" names or trademarks (like `aws-bucket123`)‚ÄîAWS discourages it.
    * Recommended to use predictable naming patterns like:
    * `project-environment-purpose`
    * Example:
        * `myapp-prod-logs`
        * `company-dev-backups`

---

* Before making anything public, first disable ‚ÄúBlock Public Access‚Äù at the bucket level.
  * Go to **S3 ‚Üí Bucket ‚Üí Permissions ‚Üí Block Public Access**
  * Turn OFF ‚ÄúBlock all public access‚Äù (only if you really want public access)
  * Save changes

* Use **Bucket Policy** to allow public read access for objects.
  * Go to **Permissions ‚Üí Bucket Policy ‚Üí Policy Generator**
  * Select:
    * **Type** ‚Üí S3 Bucket Policy
    * **Effect** ‚Üí Allow
    * **Principal** ‚Üí `*` (public)
    * **Actions** ‚Üí
      * `s3:GetObject` (allows public read of objects)
    * **ARN** ‚Üí Enter bucket ARN + `/*`
      * Example: `arn:aws:s3:::my-bucket-name/*`

* Click **Add Statement ‚Üí Generate Policy**
* Copy generated JSON and paste it into bucket policy.
* Resulting policy looks like this:
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::my-bucket-name/*"
      }
    ]
  }
  ```
  * Can apply policy with public access enabled(first enable public access)
* After applying, all objects inside the bucket become publicly readable.
* To verify:
  * Open any object ‚Üí Copy object URL
  * Paste into browser ‚Üí Should load without authentication

* Note:
  * **Bucket itself does not become public**, only the objects inside it.
  * You must enable public access intentionally, as AWS disables it by default for security reasons.

---

* S3 object URL -->
    * https://mahin-s3-bucket.s3.ap-south-1.amazonaws.com/Tech.png
    * https://mahin-s3-bucket.s3.ap-south-1.amazonaws.com/Open+OnDemand.docx?versionId=8XLPqDCVz4RpdX0E9ZJ2lWfWkTOtVMUx

<br>

* https://{bucket-name}.s3.{region}.amazonaws.com/{object-name}
* https://{bucket-name}.s3.{region}.amazonaws.com/{object-name}?versionId={VersionID}

---

* **Object Key**
  * Unique identifier (full path) of the object inside the bucket
  * Example: `images/profile/user1.jpg`

* **Object Size**
  * Size of the object in bytes.
  * Maximum object size: **5 TB**

* **ETag**
  * A hash-like identifier used to verify data integrity
  * For single-part uploads, it is usually the MD5 checksum
  * For multi-part uploads, it is not a simple MD5

* **Version ID**
  * Present only when versioning is enabled
  * Each update creates a new version ID

* **Last Modified**
  * Timestamp of when object was last updated
  * Uses UTC time

* **Storage Class**
  * Defines cost + access performance
  * Examples: Standard, Intelligent-Tiering, IA, Glacier, Deep Archive

* **Owner**
  * AWS account that owns the object
  * Shows display name and ID

* **Permissions**
  * Defines who can read/write the object
  * Includes ACL + Bucket Policy impact

* **Object URL**
  * Public or private link to access the object
  * Format: `https://bucket-name.s3.region.amazonaws.com/key`

* **Metadata**
  * Key-value pairs attached to object
  * Types:
    * System metadata (Content-Type, Content-Length)
    * User-defined metadata (custom headers)

* **Server-Side Encryption**
  * Indicates encryption applied
  * Options: SSE-S3, SSE-KMS, SSE-C
  * Shows KMS key ID if used

* **Checksum**
  * SHA1/SHA256/CRC32 for integrity validation (newer S3 features)

* **Content Type**
  * MIME type of the object
  * Example: `image/png`, `text/html`

* **Object Lock (If enabled)**
  * Protects against accidental deletion or modification
  * Modes: Governance, Compliance

* **Replication Status**
  * Shows if object is replicated to another bucket (cross-region replication)

* **Expiration / Lifecycle**
  * Shows if a lifecycle rule will delete or transition the object later

---

#### **S3 Objects Tag use cases**

* Classify objects for lifecycle management
  * Example: tag = `environment=prod` or `data_type=logs`
  * Lifecycle rules can delete/transition only tagged objects

* Apply fine-grained cost allocation
  * Tags help track cost by department, project, client, or team
  * Example: `department=finance`, `project=ml-research`
  * Visible in Cost Explorer & Billing reports

* Selective replication
  * Use tags to replicate only specific objects to another region/bucket
  * Example: CRR rule: replicate only objects tagged `critical=true`

* Security and compliance grouping
  * Tag sensitive objects like PII or internal documents
  * Apply policies or monitoring based on tags
  * Example: `classification=confidential`

* Automate workflows with Lambda
  * AWS event notifications can be triggered for specific tags
  * Example: when `status=new`, trigger Lambda to process files

* Retention control
  * Use tags to set custom retention policies
  * Example: keep `log_type=audit` for 7 years
  * keep `log_type=application` for 30 days

* Data analytics categorization
  * Use tags to categorize input files before feeding into Athena, EMR, Glue
  * Example: `dataset=training`, `dataset=test`

* Business-level organization
  * Tags allow dynamic searching and grouping without folder dependency
  * Example: `customer=12345` even if files are stored in different paths

* Selective deletion
  * Lifecycle or scripts can remove objects with specific tags
  * Useful in cleaning temporary or test data

* Troubleshooting & operational tracking
  * Tag files created by specific services, pipelines, or jobs
  * Example: `pipeline=ETL_v2` or `source=mobile-app`

---

#### **S3 object key**

* Object key is the **full path** of the object inside a bucket

  * It uniquely identifies an object
  * Format: `"folder1/folder2/filename.ext"`

* S3 has **no real folder structure**

  * ‚ÄúFolders‚Äù are only a visual representation
  * Everything is stored as a flat structure with keys
  * Example key: `images/2024/user/profile.png`

    * `images/` ‚Üí prefix
    * `2024/` ‚Üí sub-prefix
    * `user/` ‚Üí sub-prefix
    * `profile.png` ‚Üí actual file name part of key

* Object key can be **almost any string**, up to **1024 bytes** in length

  * Can include letters, numbers, special characters like `! - _ . * ' ( )`
  * Recommended to avoid spaces
  * URL encoded characters become confusing

* Object key MUST be unique inside bucket

  * If you upload another file with same key ‚Üí it replaces the previous object (unless versioning enabled)

* Object key structure creates ‚Äúprefixes‚Äù

  * Prefix = beginning of key, used for performance optimization
  * S3 organizes data based on prefixes
  * Example:

    * Prefix1: `logs/2025/01/`
    * Prefix2: `logs/2025/02/`
  * Separating workloads by prefix improves performance for parallel reads/writes

* Object key determines how the object URL looks

  * URL format:

    * `https://bucket-name.s3.region.amazonaws.com/object-key`
  * Example:

    * Key = `photos/2025/event1.jpg`
    * URL = `https://mybucket.s3.ap-south-1.amazonaws.com/photos/2025/event1.jpg`

* Object key interacts with lifecycle rules

  * You can apply lifecycle changes only to objects with specific prefixes
  * Example:

    * Delete objects under prefix `temp/` after 7 days

* Object key interacts with replication rules

  * CRR/SRR can replicate only specific prefixes
  * Example: replicate only keys starting with `backup/`

* Object key interacts with event notifications

  * Trigger Lambda/SQS/SNS only for certain prefixes/suffixes
  * Example:

    * Suffix `.jpg` triggers image processing
    * Prefix `uploads/` triggers a workflow

* Best practices for object key design

  * Avoid very long keys
  * Use predictable folder-like structure
  * Include metadata in naming:

    * Example: `customer123/logs/2025-11-10/app.log`
  * Use date-based structure for logs:

    * `logs/2025/11/10/filename`

* Object key supports versioning indirectly

  * Same key + new upload creates new version ID
  * But key remains same

---

#### **Multi-part upload**

* Multipart upload allows uploading **large objects** to S3 in multiple parts instead of a single upload request.

* Recommended / required for:

  * Files larger than **100 MB** (recommended by AWS)
  * Maximum object size supported: **5 TB**

* Each part is uploaded independently:

  * Minimum size for each part = **5 MB** (except the last part)
  * Supports parallel upload ‚Üí faster performance

* Multipart upload steps:

  * **Step 1** ‚Äì Initiate upload ‚Üí S3 returns `UploadId`
  * **Step 2** ‚Äì Upload individual parts using `UploadId`
  * **Step 3** ‚Äì Complete upload ‚Üí parts are combined into final object
  * (Optional) Abort upload ‚Üí deletes temporary parts

* Why multipart upload is fast:

  * Different parts are uploaded simultaneously (parallelism)
  * Utilizes full network bandwidth

* Advantages:

  * Faster upload for large files
  * Can retry failed parts independently ‚Üí reduces upload failure
  * Network interruptions affect only the specific part, not whole file
  * Supports client-side pause/resume functionality

* How S3 stores parts:

  * Temporarily stores uploaded parts until "CompleteMultipartUpload" is called
  * Parts are not visible as objects until upload completes
  * If aborted, parts are removed by S3

* API used:

  * `CreateMultipartUpload`
  * `UploadPart`
  * `UploadPartCopy`
  * `CompleteMultipartUpload`
  * `AbortMultipartUpload`
  * `ListMultipartUploads`

* Parallel upload example:

  * Part 1: Bytes 0‚Äì5MB
  * Part 2: Bytes 5MB‚Äì10MB
  * Part 3: Bytes 10MB‚Äì15MB
  * ‚Ä¶ uploaded concurrently

* Uploading large files through CLI:

  ```bash
  aws s3 cp bigfile.zip s3://mybucket/ --expected-size=5000000000
  ```

* Upload via AWS SDK automatically uses multipart for large files:

  * AWS SDKs (Python boto3, Java, Node.js) automatically switch to multipart upload when file size crosses threshold.

* Multipart upload with S3 Transfer Acceleration also works:

  * Speeds upload using edge locations
  * Useful for cross-region uploads

* Important internal concepts:

  * Each part gets its own **ETag**
  * Final ETag of multipart upload is *not* a simple MD5
  * Final ETag looks like:

    * `abcd1234-10` ‚Üí where `10` indicates number of parts

* Security during multipart upload:

  * Upload can be encrypted with SSE-S3, SSE-KMS, or SSE-C
  * Encryption applies after all parts get merged

* Use cases:

  * Large video files
  * Database backups
  * VM images
  * Log archives
  * Machine learning datasets

* Lifecycle impact:

  * Multipart uploads left "incomplete" cost storage
  * Set lifecycle rule to delete incomplete multipart uploads after X days

---

#### **Bucket Versioning**

* Versioning is a feature that **preserves, retrieves, and restores every version** of an object stored in an S3 bucket.

* When versioning is enabled:

  * Every new upload of the same object key creates a **new version**.
  * Old versions are **never overwritten or deleted automatically**.
  * Each version has a unique **Version ID**.

* Default behavior (versioning = disabled):

  * Overwriting an object replaces the old one permanently.
  * Deleting an object removes it permanently.

* After enabling versioning:

  * Enable ‚Üí bucket becomes versioned permanently.
  * You cannot disable it later; you can only **suspend** it.

* Object overwrite changes:

  * Old version remains in bucket with unique version ID.
  * Latest upload becomes the "current version".

* Object delete behavior:

  * Delete creates a **delete marker**, not actual deletion.
  * Old versions remain available.
  * Removing delete marker restores previous version.

* Versioning states:

  * **Disabled** ‚Äì default state (no version IDs).
  * **Enabled** ‚Äì full versioning.
  * **Suspended** ‚Äì versioning paused; new uploads get "null" version.

* Version ID:

  * Automatically assigned by AWS.
  * Uniquely identifies each version of an object.
  * For suspended buckets ‚Üí new version = "null"

* Major benefits:

  * Protects against accidental delete/overwrite

    * Recovery possible by removing delete marker
  * Supports rollback to previous version easily
  * Useful in audits and compliance scenarios

* How versioning impacts S3 storage:

  * Every version consumes storage separately.
  * If you upload a 10 MB file 10 times ‚Üí total storage = 100 MB.
  * Must use lifecycle rules to remove old versions and manage cost.

* Permissions & versioning:

  * Delete permission required to remove delete markers.
  * You can allow specific IAM roles to delete only certain versions.

* Replication with versioning:

  * Cross-Region Replication (CRR) requires versioning enabled on both buckets.
  * Same for Same-Region Replication (SRR).

* Lifecycle management and versioning:

  * Use lifecycle rules to:

    * Transition older versions to Glacier
    * Delete non-current versions after X days
    * Clean up incomplete multipart uploads

* Version listing:

  ```bash
  aws s3api list-object-versions --bucket <bucket-name>
  ```

* Restore an old version:

  * Download:

    ```bash
    aws s3api get-object --bucket mybucket --key file.txt --version-id <ID> file.txt
    ```
  * Restore to latest:

    ```bash
    aws s3 cp s3://mybucket/file.txt?versionId=<ID> s3://mybucket/file.txt
    ```

* Common use cases:

  * Application artifact storage
  * Terraform state file storage
  * Data backup
  * Log retention with rollback
  * Compliance and audit requirements

* Using AWS Console
    * Open AWS Management Console ‚Üí Go to S3.
    * Select the bucket you want to enable versioning on.
    * Go to Properties tab.
    * Scroll down to Bucket Versioning section.
    * Click Edit.
    * Choose Enable.
    * Click Save changes.

* Using AWS CLI
    ```bash
    aws s3api put-bucket-versioning \
    --bucket <bucket-name> \
    --versioning-configuration Status=Enabled
    ```

* Using Terraform
    ```hcl
    resource "aws_s3_bucket_versioning" "example" {
    bucket = aws_s3_bucket.example.id

    versioning_configuration {
        status = "Enabled"
    }
    }
    ```

* A null version ID is assigned to objects when versioning is disabled or suspended on an S3 bucket.
    ```bash
    Versioning disabled ‚Üí upload file ‚Üí file gets null version
    Enable versioning ‚Üí upload file ‚Üí file gets version A
    Upload again ‚Üí version B
    Suspend versioning ‚Üí upload again ‚Üí overwrite null version
    ```
    * How to see null version objects
        ```bash
        aws s3api list-object-versions --bucket <bucket-name>
        ```
    * Null version will show `VersionId: null`
---

#### **Delete Marker**

* A **Delete Marker** is a special placeholder object created when you delete an object from a versioned S3 bucket.
* It does **not** remove the data; it only hides the current version from user view(aws cli or console) and creates a new version with `VersionId = <delete-marker-id>` and it become a current version.
* Old versions remain intact in the bucket and can be restored/undo by removing the delete marker.
* Delete marker also consumes storage (small metadata size).
* Old versions remain ‚Üí cost increases if many deletes are made.
* If versioning is disabled, deleting a file actually deletes it permanently.
* If we delete specific version of object delete marker not create instead it permanently delete that version.
* If we delete full object without clicking on `show version` option it create a delete marker as a current object and hide object in list, when we enable `show version` we see full list of object with delete marker.
* When we delete the delete marker entire object and it's versions recovered.

* **Restoring deleted object**
* Simply remove the delete marker:
  * Not deleting actual data
  * Delete marker removal makes previous version visible again

* Example:
  ```bash
  aws s3api delete-object \
    --bucket mybucket \
    --key file.txt \
    --version-id <delete-marker-version-id>
  ```
  * After this, old version becomes visible again.


* **Behavior when versioning is suspended**


* **Behavior in cross-region replication**

* Delete markers can replicate to the target bucket depending on replication rules.
* If replication is configured to include delete markers:
  * Delete marker in source can remove object in destination.



* **Lifecycle rules interaction**
    * Lifecycle policies can:
        * Delete non-current versions
        * Delete old delete markers
        * Clean incomplete multipart uploads
    * Example rule:
        * Delete delete markers older than 30 days.

* **Permanently delete**
    * In a versioned bucket, a normal delete only creates a delete marker ‚Äî it does NOT permanently delete data.
    * To permanently delete, you must delete every version of the object‚Äîincluding delete markers.
    ```bash
    aws s3api list-object-versions --bucket mybucket --prefix file.txt #List all versions
    aws s3api delete-object --bucket mybucket --key file.txt --version-id <version-id> #Delete every version manually

    #Delete All Versions Automatically (CLI)
    aws s3api delete-objects --bucket mahin-s3-bucket --delete \
  "$(aws s3api list-object-versions --bucket mahin-s3-bucket \
     --prefix Open-OnDemand.docx --output=json \
     --query='{Objects: Versions[*].{Key:Key,VersionId:VersionId}}')"
    ```

* **When versioning is suspended**
    * Old version stay, new version have version id null and must delete al versions
---
```bash
$ aws s3api list-object-versions --bucket mahin-s3-bucket --prefix Open-OnDemand.docx
{
    "Versions": [
        {
            "ETag": "\"4a62ee3ec2939d5514d92b43ccfe3b63\"",
            "ChecksumAlgorithm": [
                "CRC64NVME"
            ],
            "ChecksumType": "FULL_OBJECT",
            "Size": 196513,
            "StorageClass": "STANDARD",
            "Key": "Open-OnDemand.docx",
            "VersionId": "FYi6HuhCSkND5BjsPdek3b98uYw5gtHJ",
            "IsLatest": true,
            "LastModified": "2025-11-10T11:02:19+00:00",
            "Owner": {
                "ID": "058628c23aec421c36b9fd9efe27ec3dbe692c0d8589ca63547d0ed5c836c3fd"
            }
        },
        {
            "ETag": "\"4a62ee3ec2939d5514d92b43ccfe3b63\"",
            "ChecksumAlgorithm": [
                "CRC64NVME"
            ],
            "ChecksumType": "FULL_OBJECT",
            "Size": 196513,
            "StorageClass": "STANDARD",
            "Key": "Open-OnDemand.docx",
            "VersionId": "null",
            "IsLatest": false,
            "LastModified": "2025-11-10T11:01:16+00:00",
            "Owner": {
                "ID": "058628c23aec421c36b9fd9efe27ec3dbe692c0d8589ca63547d0ed5c836c3fd"
            }
        }
    ],
    "RequestCharged": null,
    "Prefix": "Open-OnDemand.docx"
}
```
---

#### **Static Site Hosting**

* S3 bucket can host **static** content only
  * HTML, CSS, JS
  * Images, audio, video
  * No server-side code (no PHP, no Python, no Node.js, no API)

* Requirements for static website hosting:
  * Bucket name must match website domain if using custom domain
    * Example: `example.com` or `www.example.com`
  * Bucket must be **publicly accessible**

* Steps to enable static website hosting:
  * Open bucket ‚Üí **Properties**
  * Go to **Static website hosting**
  * Choose **Enable**
  * Select **Host a static website**
  * Set:
    * Index document ‚Üí `index.html`
    * Error document ‚Üí `error.html`
  * Save changes

* Make bucket objects public:
  * Disable Block Public Access (only for static site)
  * Add public-read bucket policy
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [
      {
        "Effect": "Allow",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::my-bucket/*"
      }
    ]
  }
  ```

* Website endpoint:
  * After enabling, S3 gives a website URL:
    ```
    http://my-bucket.s3-website-<region>.amazonaws.com
    ```
  * This endpoint supports HTTP only (not HTTPS)

* Behavior:
  * S3 serves `index.html` automatically when root is accessed
  * `404.html` is shown for missing pages

* Use cases:
  * Portfolio website
  * Documentation site
  * Blog (static generated using Hugo/Jekyll)
  * Static React/Vue build output (only if routed via CloudFront)

* Cannot do:
  * No API backend (must use Lambda/API Gateway)
  * No dynamic content (no database interaction directly)

* Access control options:
  * Public website ‚Üí requires public-read objects
  * Private website ‚Üí use CloudFront signed URLs or IAM policies

* For HTTPS:
  * S3 website endpoint does not support HTTPS
  * Use CloudFront with ACM SSL certificate

* DNS using Route 53:
  * Point domain to CloudFront
  * OR use S3 website endpoint (A record ‚Üí alias)
  * Bucket name must match domain name for direct S3 hosting

* Pricing:
  * Storage cost (per GB)
  * Request cost (GET requests for static assets)
  * Data transfer cost (outbound traffic)

---

#### Redirection

```
www.example.com ‚Üí example.com
```

**1. Create TWO S3 buckets**

* Bucket 1 ‚Üí `example.com`
    * This bucket will **host the actual website**.
    * Enable **Static website hosting** with index.html and error.html.
    * Upload your website files here.

* Bucket 2 ‚Üí `www.example.com`
    * This bucket will be used only for **redirection**.
    * DO NOT upload website files here.
    * Go to **Properties ‚Üí Static website hosting**
        * Choose **Redirect requests for an object**
        * Target bucket: `example.com`
        * Protocol: `http` or `https` (prefer **https** if using CloudFront)
    * Save changes.

**2. Bucket Name Must Match Domain**
* This is important for S3 website hosting & redirection to work properly.
    * Bucket1 = `example.com`
    * Bucket2 = `www.example.com`

**3. Allow Public Read Access**
* For both buckets:
    * Disable **Block Public Access** (only for website hosting buckets)
    * Add this policy to both buckets (change bucket name):
        ```json
        {
        "Version": "2012-10-17",
        "Statement": [
            {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::example.com/*"
            }
        ]
        }
        ```

**4. Configure Domain using Route 53**

* For `example.com`
    * Add **A record (Alias)** ‚Üí points to S3 website endpoint of `example.com`
* For `www.example.com`
    * Add **A record (Alias)** ‚Üí points to S3 website endpoint of `www.example.com`

* Visiting:
  ```
  http://www.example.com
  ```
  will return a **301 redirect** to:
  ```
  http://example.com
  ```

---

* **S3 Redirection Rules?**

    * S3 Redirection Rules are XML-based rules applied in **Static Website Hosting** configuration.
    * They work only on the **S3 website endpoint**, not the REST API endpoint.
    * They allow S3 to:
        * Redirect prefix ‚Üí another prefix
        * Redirect specific file ‚Üí another file
        * Redirect to another bucket or domain
        * Redirect with HTTP 301 or 302
        * Add/replace prefixes
        * Conditional redirect based on HTTP error codes (optional)

    * **Where to configure redirection rules**
        * Go to **S3 Console ‚Üí Bucket ‚Üí Properties**
        * Scroll down ‚Üí **Static website hosting**
        * Click ‚Üí **Edit Redirection rules**
        * Paste XML rules


    * **Basic Redirection Rule Syntax**
        ```json
            [
              {
                "Condition": {
                  "KeyPrefixEquals": "old/"
                },
                "Redirect": {
                  "ReplaceKeyPrefixWith": "new/"
                }
              }
            ]

        ```

    * **Common Use Cases + Examples**

        * Redirect ENTIRE PREFIX to Another Prefix
            * Example: `/old-folder/*` ‚Üí `/new-folder/*`

                ```json
                    [
                      {
                        "Condition": {
                          "KeyPrefixEquals": "old-folder/"
                        },
                        "Redirect": {
                          "ReplaceKeyPrefixWith": "new-folder/"
                        }
                      }
                    ]

                ```

        * Redirect Single Page to Another Page
            * Example: `/about.html` ‚Üí `/about-us.html`

                ```json
                    [
                      {
                        "Condition": {
                          "KeyPrefixEquals": "about.html"
                        },
                        "Redirect": {
                          "ReplaceKeyWith": "about-us.html"
                        }
                      }
                    ]

                ```

        * Redirect Old Prefix to External URL
            * Example: `/blog/*` ‚Üí `https://newsite.com/blog/`

                ```json
                [
                  {
                    "Condition": {
                      "KeyPrefixEquals": "blog/"
                    },
                    "Redirect": {
                      "HostName": "newsite.com",
                      "Protocol": "https"
                    }
                  }
                ]

                ```

        * Redirect Based on HTTP Error Code (Fallback Redirect)
            * Example: if 404 ‚Üí redirect to `/index.html` (good for SPA apps)

                ```json
                [
                  {
                    "Condition": {
                      "HttpErrorCodeReturnedEquals": "404"
                    },
                    "Redirect": {
                      "ReplaceKeyWith": "index.html"
                    }
                  }
                ]

                ```

        * Prefix-Based Conditional Redirect With Hostname Change
            * Example: `/docs/*` ‚Üí `https://newdomain.com/help/*`

                ```json
                [
                  {
                    "Condition": {
                      "KeyPrefixEquals": "docs/"
                    },
                    "Redirect": {
                      "HostName": "newdomain.com",
                      "Protocol": "https",
                      "ReplaceKeyPrefixWith": "help/"
                    }
                  }
                ]

                ```
        
        * All Redirect Options Explained
            * Inside `<Redirect>` you can use (combination allowed):

                | Tag                      | Purpose            |
                | ------------------------ | ------------------ |
                | `<HostName>`             | Change domain      |
                | `<Protocol>`             | http/https         |
                | `<ReplaceKeyPrefixWith>` | Replace old prefix |
                | `<ReplaceKeyWith>`       | Replace full key   |
                | `<HttpRedirectCode>`     | Set 301 or 302     |

        * Conditions You Can Use

            | Condition                       | Description                        |
            | ------------------------------- | ---------------------------------- |
            | `<KeyPrefixEquals>`             | Match keys with prefix             |
            | `<HttpErrorCodeReturnedEquals>` | Trigger redirect when error occurs |

        * Limitations
            * Only works with **S3 website endpoint**
            * Does NOT work with:
                * REST endpoint
                * CloudFront origin (unless dynamic rewrite)
            * Only supports 301/302
            * XML format only

---
```js
// ‚úÖ 1. Redirect to a New Host
// Redirects request to another domain/subdomain
{
  "Redirect": { "HostName": "new.example.com" }
}
```
```js
// ‚úÖ 2. Redirect to a Specific Protocol
// Forces redirect from http ‚Üí https or vice-versa
{
  "Redirect": { "Protocol": "https" }
}
```
```js
// ‚úÖ 3. Replace Key Prefix (prefix-to-prefix rewrite)
// Rewrites beginning of the key while keeping the rest of path dynamic
{
  "Redirect": { "ReplaceKeyPrefixWith": "blog/" }
}
```
```js
// ‚úÖ 4. Replace Key With (redirect to a single file)
// Sends ALL matched requests to EXACT one file
{
  "Redirect": { "ReplaceKeyWith": "index.html" }
}
```
```js
// ‚úÖ 5. Set HTTP Redirect Code (301 or 302)
// Choose permanent or temporary redirect
{
  "Redirect": { "HttpRedirectCode": "301" }
}
```
```js
// ‚úÖ 6. CONDITION: Match Key Prefix
// Matches paths that start with a specific prefix
{
  "Condition": { "KeyPrefixEquals": "old/" }
}
```
```js
// ‚úÖ 7. CONDITION: Match Error Code
// Matches when request returns specific error (commonly 404)
{
  "Condition": { "HttpErrorCodeReturnedEquals": "404" }
}
```
```js
// ‚úÖ 8. Key Prefix + ReplaceKeyPrefixWith
// Replace folder prefix -> dynamic path rewrite
{
  "Condition": { "KeyPrefixEquals": "docs/" },
  "Redirect": { "ReplaceKeyPrefixWith": "documentation/" }
}
```
```js
// ‚úÖ 9. Key Prefix + ReplaceKeyWith
// Redirect entire prefix to a single static file
{
  "Condition": { "KeyPrefixEquals": "content" },
  "Redirect": { "ReplaceKeyWith": "content/blog.html" }
}
```
```js
// ‚úÖ 10. Key Prefix + HostName (redirect to subdomain)
// Cross-domain redirect for certain folder
{
  "Condition": { "KeyPrefixEquals": "mobile/" },
  "Redirect": { "HostName": "m.example.com" }
}
```
```js
// ‚úÖ 11. Key Prefix + Protocol
// Force https redirection for files under this prefix
{
  "Condition": { "KeyPrefixEquals": "secure/" },
  "Redirect": { "Protocol": "https" }
}
```
```js
// ‚úÖ 12. Key Prefix + HttpRedirectCode
// Redirect prefix + change host + permanent redirect
{
  "Condition": { "KeyPrefixEquals": "legacy/" },
  "Redirect": {
    "HostName": "new.example.com",
    "HttpRedirectCode": "301"
  }
}
```
```js
// ‚úÖ 13. Error Code + ReplaceKeyPrefixWith
// 404 fallback ‚Üí rewrite prefix
{
  "Condition": { "HttpErrorCodeReturnedEquals": "404" },
  "Redirect": { "ReplaceKeyPrefixWith": "errors/" }
}
```
```js
// ‚úÖ 14. Error Code + ReplaceKeyWith
// 404 fallback ‚Üí redirect to specific file (SPA routing)
{
  "Condition": { "HttpErrorCodeReturnedEquals": "404" },
  "Redirect": { "ReplaceKeyWith": "404.html" }
}
```
```js
// ‚úÖ 15. Error Code + HostName
// If file missing ‚Üí send to backup domain
{
  "Condition": { "HttpErrorCodeReturnedEquals": "404" },
  "Redirect": { "HostName": "backup.example.com" }
}
```
```js
// ‚úÖ 16. Error Code + Protocol
// If file missing ‚Üí force https version
{
  "Condition": { "HttpErrorCodeReturnedEquals": "404" },
  "Redirect": { "Protocol": "https" }
}
```
```js
// ‚úÖ 17. Error Code + HttpRedirectCode
// If file missing ‚Üí use specified redirect code
{
  "Condition": { "HttpErrorCodeReturnedEquals": "404" },
  "Redirect": { "HttpRedirectCode": "302" }
}
```
```js
// ‚úÖ 18. Redirect Without Any Condition
// Applies to ALL requests
[
  {
    "Redirect": {
      "HostName": "new.example.com"
    }
  }
]
```
```js
// ‚úÖ 19. Redirect to different host + replace prefix
// Cross-domain + rewrite path prefix
{
  "Condition": { "KeyPrefixEquals": "old/" },
  "Redirect": {
    "HostName": "docs.example.com",
    "ReplaceKeyPrefixWith": "v2/"
  }
}
```
```js
// ‚úÖ 20. Redirect to different host + replace full key
// Every request under "blog" goes to static location on new host
{
  "Condition": { "KeyPrefixEquals": "blog" },
  "Redirect": {
    "HostName": "news.example.com",
    "ReplaceKeyWith": "index.html"
  }
}
```

---
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Welcome to My S3 Static Site</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin-top: 100px;
            background-color: #f4f4f4;
        }
        h1 { color: #333; }
        p { color: #555; }
    </style>
</head>
<body>
    <h1>Welcome to My S3 Static Website üöÄ</h1>
    <p>Your website is successfully hosted on Amazon S3.</p>
</body>
</html>
```
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>404 - Page Not Found</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin-top: 120px;
            background-color: #fafafa;
        }
        h1 { color: #d9534f; }
        p { color: #777; }
        a {
            display: inline-block;
            margin-top: 20px;
            text-decoration: none;
            background: #0275d8;
            color: white;
            padding: 10px 20px;
            border-radius: 5px;
        }
        a:hover {
            background: #025aa5;
        }
    </style>
</head>
<body>
    <h1>404 - Page Not Found</h1>
    <p>Oops! The page you are looking for doesn't exist.</p>
    <a href="/index.html">Go Back Home</a>
</body>
</html>
```
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About Us</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f8f8f8;
            color: #333;
            padding: 40px;
        }
        .container {
            max-width: 800px;
            margin: auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #222;
        }
        p {
            font-size: 18px;
            line-height: 1.6;
        }
        a {
            display: block;
            text-align: center;
            margin-top: 20px;
            text-decoration: none;
            background: #0275d8;
            color: white;
            padding: 10px 20px;
            border-radius: 5px;
        }
        a:hover {
            background: #025aa5;
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>About Us</h1>
        <p>
            Welcome to our static website hosted on Amazon S3.  
            This page gives you a simple demonstration of how multiple pages like 
            <strong>index.html</strong>, <strong>error.html</strong>, and <strong>about.html</strong>
            can exist and link to each other.
        </p>
        <p>
            S3 static website hosting is fast, reliable, and low-cost. It is perfect for hosting
            simple static webpages such as portfolios, documentation sites, landing pages,
            or basic company information.
        </p>
        <p>
            You can freely design, customize, and expand this page according to your project requirements.
        </p>

        <a href="/index.html">‚Üê Back to Home</a>
    </div>

</body>
</html>
```
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Welcome to My S3 Static Site</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            margin-top: 100px;
            background-color: #f4f4f4;
        }
        h1 { color: #333; }
        p { color: #555; }
    </style>
</head>
<body>
    <h1>Blog1</h1>
</body>
</html>
```
---
```
http://mahinraza.site.s3-website.ap-south-1.amazonaws.com/blog.html ---> http://mahinraza.site.s3-website.ap-south-1.amazonaws.com/content/blog.html
```
```json
[
    {
        "Condition": {
            "KeyPrefixEquals": "blog.html"
        },
        "Redirect": {
            "ReplaceKeyWith": "content/blog.html"
        }
    }
]
```
---

#### S3 Transfer Acceleration

  * A feature that accelerates uploads/downloads to an S3 bucket using Amazon CloudFront‚Äôs global edge network.
  * Edge Location used ---> CDN, S3 Transfer Acceleration.
  * Speeds up transfer for long-distance uploads (e.g., India ‚Üí US bucket).
  * Uses AWS edge locations to route the upload to nearest AWS region over optimized network path.
  * Requirements / Conditions
    * Bucket name cannot contain periods if you want SSL endpoint (ex: mybucket.mydomain.com ‚Üí ‚ùå).
      ```bash
      S3 Transfer Acceleration is not supported for buckets with periods (.) in their names
      ```
    * Bucket must NOT block all public access using ACL if signed URLs are used.
    * Works only with PUT, POST, GET operations.
  * How to Enable S3 Transfer Acceleration ‚Äì Console (GUI)
    * Login to AWS Console
    * Go to Amazon S3
    * Select your bucket
    * Click Properties
    * Scroll to Transfer Acceleration
    * Click Edit
    * Choose:
      * ‚úÖ Enabled
      * (Optional) Suspended ‚Üí disable but keep configuration
    * Save changes
  * Activation takes ~10‚Äì20 minutes to propagate.

  * How to Enable S3 Transfer Acceleration ‚Äì AWS CLI
    ```bash
    aws s3api put-bucket-accelerate-configuration \
    --bucket mybucket \
    --accelerate-configuration Status=Enabled

    aws s3api get-bucket-accelerate-configuration --bucket mybucket
    ```

  * Accelerated Endpoint URL
    ```bash
    http://<bucketname>.s3-accelerate.amazonaws.com
    https://<bucketname>.s3-accelerate.amazonaws.com
    https://BUCKETNAME.s3-accelerate.ap-south-1.amazonaws.com #region specific

    https://mahinraza-bucket.s3-accelerate.amazonaws.com
    ```
  * If bucket name contains dots ‚Üí SSL certificate mismatch ‚Üí use HTTP or virtual style.
  * How to Use Accelerated Transfer
    * Option 1: Use accelerated endpoint with CLI
      ```bash
      aws s3 cp bigfile.zip s3://mybucket/ --endpoint-url https://mybucket.s3-accelerate.amazonaws.com
      ```
    * Option 2: AWS SDK (Python Boto3 example)
      ```python
      import boto3

      s3 = boto3.client(
          's3',
          config=boto3.session.Config(s3={'use_accelerate_endpoint': True})
      )

      s3.upload_file('file.zip', 'mybucket', 'file.zip')
      ```
      ```python
      import boto3

      config = boto3.session.Config(s3={'use_accelerate_endpoint': True})

      s3 = boto3.client('s3', config=config)

      s3.upload_file("bigfile.zip", "mybucket-accelerated-demo", "bigfile.zip")
      ```

    * Option 3: AWS CLI with configuration file
      * Add to ~/.aws/config:
        ```bash
        [profile accelerated]
        s3 =
            use_accelerate_endpoint = true
        ```
        ```bash
        aws s3 cp file.zip s3://mybucket/ --profile accelerated
        ```
    * Upload using Accelerated endpoint (curl)
      ```bash
      A plain curl command like yours sends a raw HTTP PUT request.

      S3 requires AWS credentials signed via Signature v4 for PUT.

      That‚Äôs why S3 returns AccessDenied.

      Use AWS CLI or SDK (not plain curl) to sign the request automatically.

      Generate a pre-signed PUT URL that includes temporary credentials.

      This allows curl upload without AWS keys in command.
      ```
      ```bash

      ```

  * Test if Acceleration Helps - https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html
    * This shows speed difference between:
        * Normal endpoint
        * Accelerated endpoint

  * Pricing (Important)
    * You pay extra only for accelerated transfers:
      * ‚úÖ Uploads/downloads via accelerated endpoint
      * ‚ùå Normal endpoint remains standard price

  * Common Mistakes & Solutions
    * ‚ùå Accelerated endpoint not working
    * ‚û°Ô∏è Solution: Wait 10‚Äì20 minutes after enabling
    <br>
    * ‚ùå SSL warning
    * ‚û°Ô∏è Bucket name has dots (e.g., mahin.raza.site) ‚Üí use HTTP or rename bucket
    <br>
    * ‚ùå Upload failing with 403
    * ‚û°Ô∏è You must allow s3:PutObject permission for uploader identity
    <br>
    * ‚ùå No speed improvement
    * ‚û°Ô∏è Acceleration helps only when client is far from bucket region

  * No acceleration for:
    * List operations (LIST/HEAD)
    * Copy operations inside S3
    * Same-region transfers
    * Small files (< 1 MB)

  * Terraform ‚Äì Enable S3 Transfer Acceleration
    ```hcl
    resource "aws_s3_bucket" "mybucket" {
      bucket = "mybucket-accelerated-demo"
    }

    resource "aws_s3_bucket_accelerate_configuration" "accel" {
      bucket = aws_s3_bucket.mybucket.bucket
      status = "Enabled"
    }
    ```

  * CloudFormation ‚Äì S3 Transfer Acceleration YAML
    ```yaml
    Resources:
      MyBucket:
        Type: AWS::S3::Bucket
        Properties:
          BucketName: mybucket-accelerated-demo
          AccelerateConfiguration:
            AccelerationStatus: Enabled
    ```

  * CloudFormation ‚Äì JSON Version
    ```json
    {
      "Resources": {
        "MyBucket": {
          "Type": "AWS::S3::Bucket",
          "Properties": {
            "BucketName": "mybucket-accelerated-demo",
            "AccelerateConfiguration": {
              "AccelerationStatus": "Enabled"
            }
          }
        }
      }
    }
    ```
  * AWS CLI ‚Äì Complete Script
    ```bash
    #Enable
    aws s3api put-bucket-accelerate-configuration \
    --bucket mybucket-accelerated-demo \
    --accelerate-configuration Status=Enabled

    #Check status
    aws s3api get-bucket-accelerate-configuration \
    --bucket mybucket-accelerated-demo

    #Disable
    aws s3api put-bucket-accelerate-configuration \
    --bucket mybucket-accelerated-demo \
    --accelerate-configuration Status=Suspended
    ```

  * Example Bucket Policy (optional)
    ```json
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Effect": "Allow",
          "Principal": "*",
          "Action": [
            "s3:PutObject",
            "s3:GetObject"
          ],
          "Resource": "arn:aws:s3:::mybucket-accelerated-demo/*"
        }
      ]
    }
    ```
    * Not required if you want private bucket access.
  
---

#### addressing_style
  * addressing_style controls how AWS SDK/CLI constructs S3 bucket URLs when making API requests.
  * There are two styles:
    * virtualhost   
    * path

  * Virtual Hosted‚ÄìStyle Addressing (Recommended by AWS)
    ```bash
    https://bucket-name.s3.amazonaws.com/key
    https://mybucket.s3.ap-south-1.amazonaws.com/photos/img.png
    ```
    * Default mode for modern SDKs
    * Works with S3 Transfer Acceleration
    * Works with multi-region access points
    * Required for many S3 advanced features
    
  * Path-Style Addressing (Legacy / Deprecated)
    ```bash
    https://s3.amazonaws.com/bucket-name/key
    https://s3.ap-south-1.amazonaws.com/mybucket/photos/img.png
    ```
    * Deprecated by AWS
    * Only works for buckets created before 2020 or local testing
    * Required when:
      * Using special characters in bucket names
      * Using minio or localstack (S3-compatible systems)
    
  * Set addressing_style in AWS CLI
    * `~/.aws/config`
        ```bash
        [default]
        s3 =
        addressing_style = virtual/path
        ```


```bash
aws s3 ls s3://mybucket --endpoint-url https://mybucket.s3.amazonaws.com #virtual
aws s3 ls s3://mybucket --endpoint-url https://s3.amazonaws.com
```

---

#### SSL Certificate Wildcard Limitation

* S3 uses a wildcard SSL certificate that looks like:
    ```
    *.s3.amazonaws.com
    ```
* This wildcard only supports ONE subdomain level below "s3.amazonaws.com".
    ```
    # Bucket: mybucket
    https://mybucket.s3.amazonaws.com ‚úÖ

    # Bucket: my.bucket.name
    https://my.bucket.name.s3.amazonaws.com ‚ùå
    ```
* Works:
    ```
    one.domain.com
    ```
* Does NOT work:
    ```
    one.two.domain.com
    one.two.three.domain.com
    ```

* Solution
    ```
    # 1Ô∏è‚É£ Use path-style addressing (no wildcard issue)
    #     https://s3.amazonaws.com/my.bucket.name/object
    # 2Ô∏è‚É£ Rename bucket (BEST practice)
    #     mybucketname
    #     my-bucket-name
    # 3Ô∏è‚É£ Put S3 behind CloudFront + your own ACM certificate
    #     - custom domain works
    #     - SSL fully valid
    #     - avoids wildcard limitation
    ```


---

### Replications

* A replication rule is a configuration you create in an S3 bucket that tells AWS:
  * Which objects should be replicated
  * Where they should be replicated to
  * How they should be replicated
* A replication rule is like a set of instructions that S3 follows to copy objects from one bucket to another.

* **What a Replication Rule Includes**
  * Source bucket (automatic)
  * Destination bucket (you choose)
  * Region of destination
  * Prefix filter (optional)
  * Tag filter (optional)
  * Should delete markers replicate?
  * Should object tags replicate?
  * Storage class of replicated objects
  * IAM role for replication

* **Default Region behavior**
  * S3 by default stores objects across multiple AZs in one region (internal AWS replication).
  * This internal replication gives 11 9's durability, but this is not user-visible replication.
  * This is not SRR and CRR.
  * It's just internal AWS durability mechanism.

* **S3 Same Region Replication (SRR)**
    * SRR = Same Region Replication.
    * It replicates objects from source bucket ‚Üí destination bucket within the SAME AWS region.
    * Example:
      `Source: s3://app-logs-mumbai`
      `Destination: s3://central-backup-mumbai`
    * No cross-region transfer charges, Only replication request charges.
    * **It is asynchronous replication (not instant, usually sec‚Äìmin)**
        * "Asynchronous" = not immediate, not real-time, not blocking.
            * Time can vary:
                * Same region: ~0.1s‚Äì5s
                * Cross region: ~2s‚Äì30s (can go higher under heavy load)
        * In S3 replication (SRR/CRR), replication happens after the write operation is completed on the source bucket.
        * The source bucket confirms the write to the client before the object is copied to the destination bucket.
    * **Internal Flow**
      * Client uploads object ‚Üí S3 stores it and returns HTTP 200 OK.
      * Only after success confirmation, S3 creates an event internally.
      * Replication engine picks that event from queue.
      * Replication engine starts copying the object to target bucket in background.
      * Destination copy takes seconds ‚Üí sometimes minutes.
    * **Why S3 uses Asynchronous replication?**
      * To avoid client-side delay:
        * If S3 were synchronous, every upload would wait for cross-region transfer before finalizing.
        * That would add massive latency.
      * To ensure performance scaling:
        * S3 handles millions of replication events per second globally.
        * Asynchronous allows batching, queueing, and distributed replication.

    * Replicates only new objects after enabling replication unless `batch replication` is used.

    * **How SRR Works (Internal Workflow)**
        * Both source and destination buckets must have Versioning = Enabled.
        * AWS S3 internally generates events for every: `PUT`,`POST`,`COPY`,`DELETE`
        * S3 Replication Rule assigned to bucket:
            * Captures those events.
            * Sends them to the replication engine.
        * Replication engine scans rule ‚Üí checks prefix/tag filters ‚Üí decides if object qualifies.
        * Engine then copies object to destination bucket.
        * Destination bucket stores object with:
            * Same Key Name
            * Same Version ID (different ID actually, but versioning behaviour same)
            * Same metadata (unless replication options modify them)

    * **SRR Use Cases (Very Detailed)**
        * Compliance (same-region legal requirement)
        * Some regulations require data must stay within country boundaries but have multiple copies.
        * Log Aggregation
            * Multiple application buckets ‚Üí one central logging bucket.
            * For CloudTrail, ELB, VPC Flow Logs, WAF logs, etc.
        * Cost Optimization
            * Keep production bucket small and optimized.
            * Replicate to a cheaper storage class bucket for analytics.
        * Operational Load Separation
            * Source bucket used by production applications.
        * Replicated bucket used for:
            * Athena queries
            * EMR processing
            * Big data analytics
        * Accidental deletion protection
            * If replication is configured to NOT replicate delete markers, the destination becomes a backup of original objects.

    * Not cross-region ‚Üí can‚Äôt survive a full regional outage.

    <br>

    * **How to Configure SRR (Same-Region Replication)**
      * Example:
        * Source bucket: `my-source-bucket`
        * Destination bucket: `my-destination-bucket`
        * Both in same region: `ap-south-1`
      * Step 1: Enable Versioning on Both Buckets
        ```bash
        aws s3api put-bucket-versioning \
        --bucket my-source-bucket \
        --versioning-configuration Status=Enabled
        ```
        ```bash
        aws s3api put-bucket-versioning \
        --bucket my-destination-bucket \
        --versioning-configuration Status=Enabled
        ```
      * Step 2: Create Replication Rule
        * Using AWS Console (easy method):
            * Go to S3 ‚Üí my-source-bucket
            * Open Management tab
            * Click Replication
            * Click Add rule
            * Choose Entire bucket or Prefix filter
            * Choose Destination bucket ‚Üí my-destination-bucket (same region)
            * Select:
              * IAM role (S3 can create automatically)
              * Replicate delete markers (Yes/No)
              * Replicate tags (optional)
            * Save rule
      * Step 3: Upload Test Object to Source
        ```bash
        aws s3 cp s3-test.txt s3://my-source-bucket/
        ```
      * Step 4: Verify Replication
        ```bash
        aws s3 ls s3://my-destination-bucket/
        ```
        

<br>

* **Cross Region Replication (CRR)**
    * CRR = Cross Region Replication.
    * It replicates objects between different AWS regions.
    * Example:
      `ap-south-1 ‚Üí eu-west-1`
    * Cross-region data transfer costs, Replication request costs.

    * **How CRR Works Internally**
        * Both buckets must be versioning enabled.
        * Replication role is created:
            * Allows S3 to read source bucket.
            * Allows S3 to write to destination bucket.
        * Replication engine copies:
            * Object data
            * Object metadata
            * Object ACL
            * Tags (if enabled)
        * Destination gets:
            * NON-identical version ID
            * Overwritten metadata (as configured)
        * Delays depend on:
            * Region latency
            * Internal workload
            * S3 replication queue size

    * **CRR Use Cases**
        * Disaster Recovery (DR)
            * If entire region fails, business continues from another region.
            * Replicated bucket becomes passive DR storage.
            * Lambda or CloudFront can switch to DR copies.
        * Low-Latency Distribution
            * Users in US read from US bucket.
            * Users in Europe read from EU bucket.
            * Users in India read from APAC bucket.
        * Global Application Deployment
            * App publishes content in one place but needs global availability.
        * Regulatory/Legal Requirements
            * Some countries require primary data stored in-country and backup stored elsewhere.
        * Cross-account Replication
            * Keep replica in different AWS account for security isolation.


    * CRR costs more because data moves between regions.
    * You pay for data transfer, replication requests, and storage in both regions.
    * Only new objects replicate unless you use batch replication.
    * If the destination bucket deletes objects using lifecycle rules, it can cause conflicts with replication.
    * Replication speed is not guaranteed and may be delayed.
    * Delete operations behave differently depending on your rule settings.
    * You can choose to replicate delete markers.
    * You can also choose to stop replication of delete markers.

    * **Internal Architecture Diagram**
        ```bash
        [Source Bucket] --event--> [S3 Replication Engine] --copy--> [Destination Bucket]
            |                                |
        Versioning ON                    IAM Role
            |                                |
        PUT/POST/COPY/DELETE Events   Filtering via rules
        ```
    
    * **Replication Rule**
        ```json
        {
          "Rules": [
            {
              "ID": "CRR-logs",
              "Status": "Enabled",
              "Prefix": "logs/",
              "Destination": {
                "Bucket": "arn:aws:s3:::backup-bucket-eu",
                "StorageClass": "STANDARD_IA"
              }
            }
          ]
        }
        ```
    * **How to Configure CRR (Cross-Region Replication)**
      * Example:
        * Source bucket: `source-logs-ap ‚Üí Region = ap-south-1`
        * Destination bucket: `backup-logs-eu ‚Üí Region = eu-west-1`

      * Step 1: Enable Versioning on both buckets
      * Step 2: Add Bucket Policy on Destination Bucket
        * This allows source bucket to write into destination bucket.
        * Example (replace bucket names + account ID):
            ```json
            {
              "Version": "2012-10-17",
              "Statement": [
                {
                "Sid": "AllowReplicationFromOtherAccount",
                "Effect": "Allow",
                "Principal": { "AWS": "arn:aws:iam::<AccountA-ID>:role/s3-replication-role" },
                "Action": ["s3:ReplicateObject","s3:ReplicateDelete","s3:PutObject"],
                "Resource": "arn:aws:s3:::backup-logs-eu/*"
                }
              ]
            }
            ```
            * If SRR (same account), you don‚Äôt need cross-account policy.
      * Step 3: Create Replication Role (If Cross-Account CRR)
        * IAM role in source account must have:
            * Permission to read from source
            * Permission to write to destination

      * Step 4: Configure Replication Rule in Source Bucket
        * S3 ‚Üí source-logs-ap ‚Üí Management ‚Üí Replication ‚Üí Add rule
        * Destination Bucket: backup-logs-eu
        * Choose ‚ÄúCross-region replication‚Äù
        * Select IAM role or let S3 create
        * Save rule

      * Test CRR
        * Upload test file:
            ```bash
            aws s3 cp test.txt s3://source-logs-ap/
            ```
        * Check in destination:
            ```bash
            aws s3 ls s3://backup-logs-eu/
            ```

<br>

* **Cross-Account Replication (CRR) between two AWS accounts (Account A ‚Üí Account B)**
  * Cross-Account Replication means replicating S3 objects from a bucket in Account A to a bucket in Account B.
  * CRR (Cross-Region Replication) or SRR (Same-Region Replication) can both be used across different AWS accounts.
  * It increases security by storing a copy of data in a completely different AWS account.

  * **Why Cross-Account Replication is Used**
    * Protect against accidental or malicious deletion in one account.
    * Maintain a secure backup in a separate account (security isolation).
    * Meet compliance requirements where copies must exist in independent environments.
    * Multi-team or multi-business-unit data sharing.

  * **How Cross-Account Replication Works (Simple Flow)**
    * Source bucket (Account A) is versioning-enabled.
    * Destination bucket (Account B) is versioning-enabled.
    * S3 creates a replication IAM role in Account A.
    * This role gets permission to:
        * Read objects from source bucket.
        * Write objects to destination bucket.
    * Destination bucket policy allows Account A‚Äôs replication role to write objects.

  * **Required Permissions (Simple Explanation)**
    * Account A grants S3 permission to:
      * Read objects from source.
      * Access KMS keys (if using SSE-KMS).
    * Account B grants Account A‚Äôs replication role permission to:
      * PutObject in destination bucket.
      * PutObjectAcl for metadata/ACL replication.

  * **How to Setup**
    * Source bucket:
        * Name: `source-bucket`
        * Account: `Account A`
    * Destination bucket:
        * Name: `destination-bucket`
        * Account: `Account B`

  * Step 1: Enable Versioning on both buckets
    * In Account A (source):
      ```bash
      aws s3api put-bucket-versioning \
        --bucket source-bucket \
        --versioning-configuration Status=Enabled
      ```
    * In Account B (destination):
      ```bash
      aws s3api put-bucket-versioning \
        --bucket destination-bucket \
        --versioning-configuration Status=Enabled
      ```

  * Step 2: Add Bucket Policy on Destination Bucket (Account B)
    * This policy allows Account A‚Äôs replication role to write objects.
    * Replace:
      `111111111111` = Account A ID
      `destination-bucket` = your bucket name
        ```json
        {
        "Version": "2012-10-17",
        "Statement": [
            {
            "Sid": "AllowAccountAS3Replication",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111111111111:role/s3-replication-role"
            },
            "Action": [
                "s3:ReplicateObject",
                "s3:ReplicateDelete",
                "s3:ReplicateTags",
                "s3:PutObject"
            ],
            "Resource": "arn:aws:s3:::destination-bucket/*"
            }
        ]
        }
        ```
    
    * Step 3: Create Replication Role in Account A
      * This IAM role lets S3 read from Account A bucket and write to Account B bucket.
      * Trust Policy
        ```json
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": { "Service": "s3.amazonaws.com" },
              "Action": "sts:AssumeRole"
            }
          ]
        }
        ```
      * Permissions Policy
        ```json
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": [
                "s3:GetObjectVersionForReplication",
                "s3:GetObjectVersionAcl",
                "s3:GetObjectVersionTagging"
              ],
              "Resource": "arn:aws:s3:::source-bucket/*"
            },
            {
              "Effect": "Allow",
              "Action": [
                "s3:ReplicateObject",
                "s3:ReplicateDelete",
                "s3:ReplicateTags",
                "s3:PutObject"
              ],
              "Resource": "arn:aws:s3:::destination-bucket/*"
            }
          ]
        }
        ```

    * Step 4: Configure Replication Rule in Source Bucket (Account A)
      * Using console:
        * `Go to S3 ‚Üí source-bucket ‚Üí Management ‚Üí Replication ‚Üí Add rule`
        * Choose:
            * Entire bucket or prefix
            * Destination bucket ARN (from Account B)
        * Select IAM role created above
        * Enable S3 Replica KMS permissions if using SSE-KMS

    * Step 5: Verify replication is working
      * Upload object to source bucket:
        ```bash
        aws s3 cp test.txt s3://source-bucket/test.txt
        ```
      * Wait 5‚Äì30 seconds.
      * Check bucket in Account B:
        ```bash
        aws s3 ls s3://destination-bucket/
        ```


* **Why Versioning Is Required for Replication?**
  * Versioning gives every object a unique version ID.
  * Replication works on object versions, NOT on raw objects.
  * When you upload a file, S3 creates a versioned copy.
  * Replication engine needs this version ID to know:
    * Which version to replicate
    * When version changed
    * When delete marker added
  * Without version IDs ‚Üí S3 cannot track which object change needs to be replicated.

* **Destination Storage Class**
  * This defines which storage class the replicated objects should use in the destination bucket.
  * Examples:
    * STANDARD
    * STANDARD_IA
    * INTELLIGENT_TIERING
    * GLACIER
    * GLACIER DEEP ARCHIVE
  * Replicated objects can be stored in a cheaper storage class in the destination.
  * Source object storage class doesn't change.

* **Replication Time Control (RTC)**
  * RTC = Replication Time Control
  * It guarantees:
    * 99.99% of new objects replicated within 15 minutes
  * This is useful for:
    * Regulatory environments
    * Critical backups
    * Enterprise SLAs
  * Without RTC
    * Replication is async and timing is unpredictable.
  * With RTC
    * AWS charges extra for RTC.

* **Replication Metrics**
  * Enables CloudWatch replication metrics for monitoring.
  * You can see:
    * Pending replication count
    * Pending replication size
    * Max time objects are waiting for replication
    * Replication failures
  * Use case:
    * Monitoring replication for compliance.
    * Troubleshooting delays.
  * Note:
    * CloudWatch metrics cost money.

* **Delete Marker Replication**
  * In versioned buckets, delete operation = create DELETE MARKER.
  * This setting controls whether the delete marker replicates.
  * If enabled:
    * Delete markers created by user delete actions will replicate.
    * Means: deleting file in source deletes in destination also.
  * If disabled:
    * The destination bucket will keep the file, even if deleted in source ‚Üí acts like a backup.
  * Important:
    * Delete markers created by lifecycle rules DO NOT replicate.

* **Replica Modification Sync**
  * Allows replicating metadata changes of the replicated objects.
  * Example:
    * If you change metadata in the source after replication:
      * Without this option ‚Üí destination object metadata does NOT update.
      * With this option ‚Üí destination metadata updates to match source.
  * What metadata?
    * ACLs
    * Tags
    * Storage class changes
    * Object locks
  * When used:
    * When you need source and destination to stay identical.

  
---

### S3 Server Access Logging
  * It logs all requests made to your S3 bucket.
  * Logs are delivered to another bucket as text files.
  * Logs take time to reflect on destination bucket.
  * Logs include:
    * Request type (GET, PUT, DELETE)
    * Requester IP
    * Time of request
    * Bucket name
    * Object key
    * HTTP status code
    * Error codes

  * Important Rules Before Enabling
    * Logging destination bucket must be in the same AWS Region.
    * Source bucket should NOT write logs to itself (to avoid infinite logging loop).
    * Destination bucket should not have lifecycle rules that delete logs too early.

  * How to Enable S3 Logging (GUI Method)
    * Step 1 ‚Äî Create/Choose a Dedicated Destination Bucket for Logs
    * Step 2 ‚Äî Go to Source Bucket
      `Properties tab --> Server Access Logging --> Edit --> Select destination bucket (log bucket) --> Prefix(s3-access-logs/) --> Save Changes

  * CLI Method
    ```bash
    aws s3api put-bucket-logging \
    --bucket my-prod-bucket \
    --bucket-logging-status '{
      "LoggingEnabled": {
        "TargetBucket": "my-application-logs",
        "TargetPrefix": "s3-logs/"
      }
    }
    ```
  
  * Destination bucket must allow source bucket to write logs.
    ```json
    {
      "Version": "2008-10-17",
      "Statement": [
        {
          "Sid": "S3ServerAccessLogsPolicy",
          "Effect": "Allow",
          "Principal": { "AWS": "*" },
          "Action": "s3:PutObject",
          "Resource": "arn:aws:s3:::my-application-logs/s3-logs/*",
          "Condition": {
            "StringEquals": { "s3:x-amz-acl": "bucket-owner-full-control" }
          }
        }
      ]
    }
    ```
  
  * Logs Will Appear
    ```bash
    my-application-logs/
      s3-access-logs/
        2024-11-10-10-00-12345ABCDEF
        2024-11-10-11-30-98765XYZPQR
    ```

  * Log File Format Example
    ```bash
    79a5a-examplebucket [10/Nov/2024:11:28:15 +0000] 1.2.3.4 GET /photos/image.jpg 200 -
    ```
    * Details include:
      * Request time
      * Source IP
      * Operation (GET/PUT)
      * Object key
      * HTTP status code
      * User-agent

  * Use cases S3 Logging
    * Troubleshooting access issues
    * Auditing user activity
    * Detect unauthorized access attempts
    * Security compliance
    * Analyzing usage patterns

---

### Storage Class

| Storage Class                  | Access Frequency | Availability | Durability             | Storage Cost | Retrieval Cost | Retrieval Time | Min Storage Days | Use Case                            |
| ------------------------------ | ---------------- | ------------ | ---------------------- | ------------ | -------------- | -------------- | ---------------- | ----------------------------------- |
| **STANDARD**                   | Frequent         | 99.99%       | 99.999999999% (11 9‚Äôs) | High         | Low            | ms             | None             | Websites, apps, high traffic data   |
| **STANDARD-IA**                | Infrequent       | 99.9%        | 11 9‚Äôs                 | Lower        | Higher         | ms             | 30 days          | Backups, DR, rarely accessed data   |
| **ONE ZONE-IA**                | Infrequent       | 99.5%        | 11 9‚Äôs                 | Lower        | Higher         | ms             | 30 days          | Non-critical, recreateable data     |
| **GLACIER Instant Retrieval**  | Rare/Infrequent  | 99.9%        | 11 9‚Äôs                 | Low          | Higher         | ms             | 90 days          | Archive with fast access            |
| **GLACIER Flexible Retrieval** | Rare             | 99.99%       | 11 9‚Äôs                 | Very Low     | High           | Minutes‚ÄìHours  | 90 days          | Long-term archive, compliance       |
| **GLACIER Deep Archive**       | Very Rare        | 99.99%       | 11 9‚Äôs                 | Lowest       | Highest        | Hours‚ÄìDays     | 180 days         | Long-term preservation (7‚Äì10 years) |

<br>

##### **Storage Class ‚Äì Detailed Explanation**
* A storage class defines **how Amazon S3 stores your data**, including redundancy level, speed, resilience, and cost.
* Each class is optimized for a specific type of workload:
  * High-performance frequent-access data
  * Infrequent-access data
  * Cold archive data
  * Deep archival/compliance storage
* Examples include: Standard, Standard-IA, One Zone-IA, Glacier Instant, Glacier Flexible, Glacier Deep Archive.
* Storage class affects:
  * How many AZs your data is stored in
  * Retrieval speed
  * Pricing model
  * Minimum retention requirements

<br>

##### **Access Frequency ‚Äì Detailed Explanation**

* Defines **how often you plan to access the data** stored in that class.
* Classes are optimized based on expected usage:
  * Frequent access ‚Üí High-speed access ‚Üí Higher storage cost
  * Infrequent access ‚Üí Cheaper storage ‚Üí Higher retrieval cost
  * Rare/Very rare access ‚Üí Deep archive ‚Üí Very cheap storage ‚Üí Very slow retrieval
* Measures expected access pattern:
  * Daily / weekly (Standard)
  * Monthly / quarterly (Standard-IA)
  * Yearly / never (Glacier classes)

<br>

##### **Availability ‚Äì Detailed Explanation**
* Availability means **how often the service is ready to serve read/write requests**.
* Measured as a percentage uptime (SLA per year or per month).
* Example: 99.99% availability means the service can be unavailable for only:
  * Around 52 minutes per year.
* Higher availability = more resilient to AZ failures.
* Standard-IA, One Zone-IA, and Glacier classes have slightly lower availability because they focus on cost efficiency.
* Glacier Deep Archive has high availability but slow retrieval.

<br>

##### **Durability ‚Äì Detailed Explanation**
* Durability measures the **probability of object loss** over a year.
* S3 provides **11 nines durability (99.999999999%)** for almost all classes.
* Achieved through:
  * Multiple copies stored across AZs (except One Zone-IA)
  * Internal integrity checks
  * Automatic data repair
* Durability is about **long-term data safety**, not accessibility.
* Even Glacier classes keep the same durability level.

<br>

##### **Storage Cost ‚Äì Detailed Explanation**
* How much you pay **per GB per month** for keeping data in that class.
* Standard is most expensive because it offers:
  * Highest availability
  * Highest performance
  * Multi-AZ redundancy
* IA and One Zone-IA lower cost because:
  * Assumes less frequent access
  * One Zone-IA stores data only in one AZ
* Glacier classes have extremely low storage cost because:
  * Designed for long-term archival
  * Not meant for active usage
  * Retrieval is slow and cost is high

<br>

##### **Retrieval Cost ‚Äì Detailed Explanation**

* Cost you pay when you **download or access** your data.
* Classes optimized for infrequent/rare access charge per GB retrieved.
* Standard retrieval is free or very cheap.
* Standard-IA retrieval cost higher than Standard.
* Glacier Instant retrieval cost similar to IA or slightly more.
* Glacier Flexible and Deep Archive have highest retrieval costs because:
  * They are designed for rare access
  * Retrieval is treated as "special job"

<br>

##### **Retrieval Time ‚Äì Detailed Explanation**
* How fast you can access or download your object.
* Standard, IA, One Zone-IA, Glacier Instant: **millisecond access**
* Glacier Flexible Retrieval:
  * Expedited: 1‚Äì5 minutes
  * Standard: 3‚Äì5 hours
  * Bulk: 5‚Äì12 hours
* Glacier Deep Archive:
  * Standard: 12 hours
  * Bulk: up to 48 hours
* Retrieval time directly affects:
  * Emergency data recovery
  * Backup restore planning
  * Regulatory requirements

<br>

**Minimum Storage Days ‚Äì Detailed Explanation**
* Minimum number of days AWS charges for storing an object in a storage class, even if you delete it earlier.
* Helps AWS maintain consistency of internal archival storage.
* Standard = no minimum
* Standard-IA = 30 days
* One Zone-IA = 30 days
* Glacier Instant Retrieval = 90 days
* Glacier Flexible Retrieval = 90 days
* Glacier Deep Archive = 180 days
* If you delete the object early ‚Üí AWS still bills full 30/90/180 days.

<br>

##### **Use Case ‚Äì Detailed Explanation**

* Defines the practical real-world scenario where that class is ideal.
* Standard:
  * Active websites, real-time analytics, user uploads, streaming content
* Standard-IA:
  * Backups, disaster recovery, less-frequent production assets
* One Zone-IA:
  * Temporary data, caches, staging areas, recreate-able datasets
* Glacier Instant Retrieval:
  * Medical records, archived logs, project artifacts requiring fast restore
* Glacier Flexible Retrieval:
  * Legacy data, compliance archives accessed once/twice per year
* Glacier Deep Archive:
  * Long-term regulatory data retention (7‚Äì10 years or more)

---

### Configure Storage Class During Object Upload
* When uploading a file manually:
  * Go to AWS Console ‚Üí S3
  * Open your bucket
  * Click Upload
  * Add files
  * Scroll down to "Properties"
  * Under "Storage Class", choose the class:
    * Options include:
      * S3 Standard
      * S3 Standard-IA
      * S3 One Zone-IA
      * S3 Intelligent-Tiering
      * S3 Glacier Instant Retrieval
      * S3 Glacier Flexible Retrieval
      * S3 Glacier Deep Archive
      * Click Upload

* **Configure Storage Class During Upload (CLI)**
  ```bash
  aws s3 cp file.txt s3://mybucket/ --storage-class STANDARD_IA
  ```

* **Change Storage Class After Upload (via Lifecycle Rule)**
  * AWS doesn‚Äôt allow directly changing storage class of an existing object; but you can do it with a Lifecycle Rule.
  * Or can use Intelligence tiering.

* **Change Storage Class Using S3 Replication (SRR/CRR)**
* **Change Storage Class with AWS S3 Console (Bulk Change)**
  * Since AWS console does not allow manually changing class for existing files:
    * Use ‚ÄúCopy‚Äù option:
      * Select object
      * Click ‚ÄúActions ‚Üí Copy‚Äù
      * Paste back to same bucket/path
      * In settings, choose desired storage class
    * This recreates object with new storage class.
    * We can also change:
      * Tags
      * ACL
      * Metadata
      * Server-side encryption
      * Checksums

---

### Lifecycle Management

* Lifecycle Management is an automated mechanism in Amazon S3 to manage object storage cost and retention.
* You define rules that automatically:
  * Move objects to cheaper storage classes (Transition)
  * Delete objects after certain time (Expiration)
  * Remove old versions
  * Remove delete markers
  * Clean up incomplete multipart uploads

* Why?
  * Helps reduce overall S3 storage cost automatically
  * Manages data lifecycle without manual actions
  * Ensures compliance by retaining data for required time
  * Organizes long-lived data and archives old, unused files
  * Saves time and avoids manual intervention

* **Components of Lifecycle Rule**
  * Rule name
  * Filter (Prefix filter like logs/ or Tag filter like env=prod)
  * Transition actions
  * Expiration actions
  * Applies to current versions
  * Applies to previous versions
  * Review rule impact

* **Where Lifecycle Rules Apply**
  * Entire bucket
  * Specific folder (prefix)
  * Objects with specific tags
  * Current object versions
  * Non-current object versions

* **Supported Storage Class Transitions**
  * Higher class to lower class only
    * Standard ‚Üí Intelligent Tiering
    * Intelligent Tiering ‚Üí (automatic tiering)
    * Standard ‚Üí Standard-IA
    * Standard ‚Üí One Zone-IA
    * Standard ‚Üí Glacier Instant
    * Standard ‚Üí Glacier Flexible
    * Standard ‚Üí Glacier Deep Archive
    * Standard-IA ‚Üí Glacier classes
    * Glacier Instant ‚Üí Glacier Flexible / Deep Archive (rare)

* **Minimum Storage Days Restrictions**
  * Standard-IA ‚Üí 30 days
  * One Zone-IA ‚Üí 30 days
  * Glacier Instant ‚Üí 90 days
  * Glacier Flexible ‚Üí 90 days
  * Glacier Deep Archive ‚Üí 180 days

* **Lifecycle Rule Actions**
  * Lifecycle rules in Amazon S3 support five major actions. These actions automatically manage objects based on time, age, prefix, or tags.
    * Transition current versions of objects between storage classesn ‚Üí Move to cheaper storage class
    * Transition noncurrent versions of objects between storage classes
    * Expire current versions of objects ‚Üí Delete old versions
    * Permanently delete noncurrent versions of objects
    * Delete marker removal ‚Üí Remove unnecessary delete markers
    * Abort multipart uploads ‚Üí Clean incomplete uploads
  
  * **Transition Actions**
    * Transition = Move objects to a different storage class after a specific number of days.
    * Automatically changes object storage class from expensive ‚Üí cheaper.
    * Helps optimize cost for older, less-accessed objects.
    * Minimum storage days apply (30/90/180 depending on class)
    * Transition is asynchronous (may take hours to apply)
    * Examples:
      * Move to Standard-IA after 30 days
      * Move to Glacier Instant Retrieval after 60 days
      * Move to Glacier Flexible Retrieval after 90 days
      * Move to Deep Archive after 180 days

  * **Expiration Actions**
    * Expiration = Delete objects automatically after a time period.
    * Removes current versions of objects permanently.
    * Frees up space and reduces cost.
    * Helps enforce retention policies.
    * Works only on eligible objects based on prefix/tag filters
    * Deleted objects cannot be restored
    * Examples:
      * Delete objects after 365 days
      * Delete logs after 30 days

  * **Noncurrent Version Expiration**
    * When versioning is enabled, an object may have multiple versions.
    * This action deletes old/non-current versions.
    * Controls cost by cleaning up previous versions.
    * Keeps version count low.
    * Useful in buckets where versioning creates many backup versions.
    * Examples:
      * Delete non-current versions after 30 days
      * Keep only the last 2 versions

  * **Delete Marker Removal**
    * Delete marker appears when you delete an object in a versioned bucket.
    * Removes unnecessary delete markers.
    * Helps clean up bucket clutter.
    * Makes deleted objects visible again if required.
    * Examples:
      * Remove delete marker after 30 days
    * Only applies to versioned buckets
    * If current version is a delete marker ‚Üí lifecycle can remove it
    * Lifecycle does not remove delete marker created by replication rules (unless configured)

  * **Abort Incomplete Multipart Uploads**
    * When an upload is started using multipart upload but not completed, fragments remain and cost money.
    * Cleans up unfinished/incomplete multipart uploads.
    * Prevents cost leakage due to abandoned uploads.
    * Examples:
      * Abort incomplete uploads after 7 days
    * Excellent for ETL workloads, data pipelines, big data jobs
    * Avoids waste of storage due to partially uploaded chunks
    * Saves money automatically

* **S3 Lifecycle Use Cases**

| Use Case                              | Initial Storage                            | Transition 1                         | Transition 2                 | Final Action                                    | Purpose                                    |
| ------------------------------------- | ------------------------------------------ | ------------------------------------ | ---------------------------- | ----------------------------------------------- | ------------------------------------------ |
| **Log Archive**                       | Keep logs in Standard for 30 days          | After 30 days ‚Üí Standard-IA          | After 90 days ‚Üí Glacier      | After 180 days ‚Üí Delete                         | Reduce cost of old logs and auto-clean     |
| **Backup Management**                 | Store daily DB backups in Standard         | After 7 days ‚Üí IA                    | After 90 days ‚Üí Deep Archive | Delete after 1 year                             | Manage backup lifecycle + cost reduction   |
| **Version Control Cleanup**           | Store latest version in Standard           | Older versions transition not needed | ‚Äî                            | Delete old versions + remove delete markers     | Reduce storage cost in versioned buckets   |
| **Cost Optimization for Big Buckets** | Keep current objects in Standard initially | Move cold objects to IA              | ‚Äî                            | Clean incomplete multipart uploads after 7 days | Optimize storage + remove unused fragments |
| **Compliance Archiving**              | Store initial files in Standard            | Optional IA                          | Move to Glacier/Deep Archive | Retain for minimum 7 years                      | Meet legal retention requirements          |


* **Object Size" Filter**
  * When creating a lifecycle rule in Amazon S3, you can control which objects the rule applies to based on their size.
  * This means the lifecycle rule will only run on objects that fall within the size limits you define.
  * Both minimum and maximum size filters can be used together--> Object must be between min and max size.
  * Size filters do NOT delete objects; they only decide which objects to include in the rule.
  * **Minimum Object Size**
    * You can specify a minimum size value (in KB, MB, or GB).
    * The lifecycle rule will only apply to objects larger than or equal to this minimum size.
    * Example:
      * Minimum size = 128 KB
      * Objects <128 KB will NOT transition to IA or Glacier classes.
    * Why this is important?
      * S3 Standard-IA and One Zone-IA charge minimum billing for objects 128 KB or larger.
      * Smaller objects are charged as 128 KB automatically.
      * To avoid wasting money, you can restrict lifecycle transitions to only large objects.
  * **Maximum Object Size**
    * You can specify a maximum object size (in KB, MB, GB).
    * The lifecycle rule will only apply to objects smaller than or equal to this maximum size.
    * Why is this used?
      * Extremely large objects may:
        * Require expensive retrieval from Glacier
        * Have long restore times
        * Be frequently accessed in real-time pipelines
        * So you may want to keep large objects in Standard or IA class instead of Glacier.

  * **Real-Life Use Cases**

    | Use Case                                          | Size Filter Type        | Size Value                      | Behavior                                                                | Outcome                                                           |
    | ------------------------------------------------- | ----------------------- | ------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------- |
    | **Avoid IA charges on small objects**             | Minimum size            | **128 KB**                      | Lifecycle applies only to objects ‚â•128 KB                               | Small objects stay in Standard ‚Üí No unnecessary IA billing        |
    | **Avoid Glacier retrieval delays for huge files** | Maximum size            | **1 GB**                        | Lifecycle applies only to objects ‚â§1 GB                                 | Large files stay in IA/Standard ‚Üí Medium-size files go to Glacier |
    | **Manage different types of data**                | Min + Max size combined | Example: Min: 100 KB, Max: 2 GB | Small files stay in Standard; medium files to Glacier; huge files to IA | Optimized cost + performance across all file sizes                |

---

### Intelligent-Tiering
* S3 Intelligent-Tiering is a smart storage class that automatically analyze and  moves objects between tiers based on access patterns.
* Designed to reduce storage cost without impacting performance.
* Ideal for unpredictable access patterns where you don‚Äôt know how often a file will be accessed.
* How It Works
  * S3 automatically monitors object access.
  * If object is frequently accessed ‚Üí stays in frequent tier.
  * If object becomes inactive ‚Üí moves to infrequent tier.
  * If object is not accessed for long ‚Üí moves to archive tiers (optional).
  * Movement is fully automated; no lifecycle rule required.
  * No retrieval fee for frequent and infrequent tiers.
* No retrieval fees
  * Frequent and IA tiers have zero retrieval fees.
  * Glacier-like fees apply only for Archive tiers.
* Changes:
  * Intelligent-Tiering has a monthly monitoring fee per object.
  * But for large objects using unpredictable access, it saves more cost than the fee.
  * AWS does NOT charge monitoring fee for objects <128 KB.
* No minimum storage duration
  * Unlike Standard-IA, Glacier, Deep Archive‚Ä¶
  * Intelligent-Tiering has NO minimum retention charge.
  * You can delete object any time ‚Üí no penalty.
* Storage Tiers Inside Intelligent-Tiering
  * Intelligent-Tiering has 6 internal tiers (automated movement):
    * Frequent Access Tier
      * For frequently accessed data
      * Highest performance
    * Infrequent Access Tier
      * For occasional access
      * Lower cost
    * Archive Instant Access Tier
      * Millisecond access
      * Lower cost than IA
    * Archive Access Tier (optional)
      * Retrieval in minutes ‚Üí hours
      * Very low cost
    * Deep Archive Access Tier (optional)
      * Retrieval in hours ‚Üí days
      * Lowest cost
    * Small Object Opt-Out Tier
      * For objects <128 KB

* **Benefits**
  * Millisecond access for Frequent, IA, and Archive-Instant tiers.
  * Only Archive & Deep Archive have slow retrieval.
  * Fast automatic transitions‚Äîno performance impact.
  * PUT/GET performance unchanged, same as S3 Standard.
  * High throughput‚Äîhandles millions of objects efficiently with no throttling.

* **Intelligent-Tiering Retrieval Cost**

| Intelligent-Tiering Tier        | Download Cost | Retrieval Speed | When Object Moves Here                                 | What It Means                                                   |
| ------------------------------- | ------------- | --------------- | ------------------------------------------------------ | --------------------------------------------------------------- |
| **Frequent Access Tier**        | ‚úÖ FREE        | ‚ö° Milliseconds  | New/recently accessed object                           | You can download anytime without extra cost                     |
| **Infrequent Access (IA) Tier** | ‚úÖ FREE        | ‚ö° Milliseconds  | After object is unused for ~30 days                    | No extra download charges even if object becomes cold           |
| **Archive Instant Access Tier** | ‚ùå Costs money | ‚ö° Milliseconds  | After long inactivity (Intelligent-Tiering auto-moves) | Retrieval has *Glacier-like cost* even though access is instant |
| **Archive Access Tier**         | ‚ùå Costs money | ‚è≥ Minutes‚ÄìHours | After extended inactivity                              | Slow + charged retrieval similar to Glacier retrieval           |
| **Deep Archive Access Tier**    | ‚ùå Costs money | üïí Hours‚ÄìDays   | After very long inactivity                             | Cheapest storage but most expensive/slow retrieval              |

<br>

---

### CORS(Cross-Origin-Resource-Sharing)

* https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html

* It is a **browser-based security mechanism** that controls which external domains are allowed to access resources (objects) in your S3 bucket.
* S3 uses CORS rules to tell the browser **which origins are allowed**, **which headers are allowed**, and **which HTTP methods can be used**.

* **Why CORS is Needed for S3?**
* By default:
  * Browser blocks requests from **one domain** to **another domain** (cross-origin protection)
  * Example:
    Website running on `https://mywebsite.com` tries to load file from `https://my-bucket.s3.amazonaws.com` ‚Üí BLOCKED without CORS.


* **Where CORS Applies**
  * Only applies to **requests from browsers** (client-side JavaScript, HTML, React, Angular, etc.)
  * Does NOT apply to:
    * CLI
    * Server-side apps (Node.js, Python backend)
    * Lambda functions
    * SDK running on server

* **Common Scenarios Where CORS Is Required**
  * React/Angular/Vue app loading images from S3
  * Website using S3 for fonts, JavaScript files, CSS
  * Front-end uploading files directly to S3
  * CloudFront ‚Üí S3 origin with custom headers
  * Web app fetching JSON files from S3

* **How CORS Works (Simplified Flow)**
  * Browser sends a **request** to S3
  * S3 checks bucket‚Äôs **CORS rules**
  * If allowed origin ‚Üí S3 sends **Access-Control-Allow-Origin** header
  * Browser allows the response
  * If not allowed ‚Üí Browser blocks access (403 CORS error)

* **Preflight Requests (Deep Explanation)**
  * Many CORS requests use **OPTIONS** method first:
  * Browser sends OPTIONS request ‚Üí "preflight"
  * Asks S3:
    * ‚ÄúIs this domain allowed?‚Äù
    * ‚ÄúIs this header allowed?‚Äù
    * ‚ÄúIs PUT allowed?‚Äù
  * S3 responds with allowed rules
  * Browser then performs the actual request (GET/PUT/POST)

* **CORS Configuration Elements**
* CORS rules in S3 consist of:
  * **AllowedOrigins**
  * **AllowedMethods**
  * **AllowedHeaders**
  * **ExposeHeaders**
  * **MaxAgeSeconds**

* **Detailed Explanation of Each Element**
  * **AllowedOrigins**
    * Defines which domains can access your bucket.
    * Examples:
      * `"*"` ‚Üí allow all domains
      * `"https://myapp.com"` ‚Üí allow specific website
      * `"https://dev.myapp.com"` ‚Üí allow only dev environment


  * **AllowedMethods**
    * Defines which HTTP methods are permitted:
      * GET
      * POST
      * PUT
      * DELETE
      * HEAD

  * **AllowedHeaders**
    * Defines what browser-requested headers are allowed.
    * Examples:
      * `"*"` ‚Üí allow all headers
      * `"Content-Type"`
      * `"Authorization"`

  * **ExposeHeaders**
    * Defines which headers browser can read in response.
    * Example:
      * `"x-amz-meta-custom"`

  * **MaxAgeSeconds**
    * Defines how long browser can cache the preflight (OPTIONS) response.
    * Example:
      * `300` seconds = 5 minutes

* **Use Cases**
  * Allow website to upload + download objects from S3:
    ```json
      {
        "CORSRules": [
          {
            "AllowedOrigins": ["https://mywebsite.com"],
            "AllowedMethods": ["GET", "PUT", "POST"],
            "AllowedHeaders": ["*"],
            "ExposeHeaders": ["ETag"],
            "MaxAgeSeconds": 3000
          }
        ]
      }
      ```

  * For Testing
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["*"],
          "AllowedMethods": ["GET"]
        }
      ]
    }
    ```

  * Upload from Browser (PUT/POST)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["*"],
          "AllowedMethods": ["PUT", "POST"],
          "AllowedHeaders": ["*"]
        }
      ]
    }
    ```

  * Allow GET Only (Public Read / CDN / Static Content)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["*"],
          "AllowedMethods": ["GET"],
          "MaxAgeSeconds": 3000
        }
      ]
    }
    ```
    * **Use Case:** Viewing images, fonts, static files
    * **Risk:** Public GET allowed ‚Äì use for public website buckets only


  * Allow Specific Website Full Access (Most Common Production Rule)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["https://mywebsite.com"],
          "AllowedMethods": ["GET", "PUT", "POST", "DELETE"],
          "AllowedHeaders": ["*"],
          "ExposeHeaders": ["ETag"],
          "MaxAgeSeconds": 3000
        }
      ]
    }
    ```
    * **Use Case:** Frontend uploading + downloading objects
    * **Risk:** Only one trusted domain can access ‚Üí Safe


  * Allow Multiple Domains (Staging + Production)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": [
            "https://mywebsite.com",
            "https://staging.mywebsite.com"
          ],
          "AllowedMethods": ["GET", "PUT", "POST"],
          "AllowedHeaders": ["*"]
        }
      ]
    }
    ```
    * **Use Case:** Testing + Production environments
    * **Risk:** Unintended domains should NOT be added

  * Allow Browser Uploads from Anywhere (Testing Only)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["*"],
          "AllowedMethods": ["PUT", "POST"],
          "AllowedHeaders": ["*"]
        }
      ]
    }
    ```
    * **Use Case:** Web-based file upload testing
    * **Risk:** Should NOT be used in production

  * Allow Preflight (OPTIONS) for Advanced Web Apps
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["https://app.example.com"],
          "AllowedMethods": ["GET", "PUT", "POST", "OPTIONS"],
          "AllowedHeaders": ["Authorization", "Content-Type", "x-amz-meta-custom"],
          "ExposeHeaders": ["ETag"],
          "MaxAgeSeconds": 600
        }
      ]
    }
    ```
    * **Use Case:** React/Angular/SPA apps that upload directly to S3
    * **Risk:** Must restrict origins

  * Allow CloudFront Origin Access (for Private S3 via CloudFront)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["https://cdn.mywebsite.com"],
          "AllowedMethods": ["GET", "HEAD"],
          "AllowedHeaders": ["*"]
        }
      ]
    }
    ```
    * **Use Case:** CloudFront ‚Üí S3 origin
    * **Risk:** Use custom domain instead of *

  * Strict Minimal CORS (Highly Secure)
    Allow only GET from specific domain with no headers.
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["https://secure.mycompany.com"],
          "AllowedMethods": ["GET"]
        }
      ]
    }
    ```
    * **Use Case:** Extremely locked down static content
    * **Risk:** No additional features

  * API JSON File Access (Allow CORS for JSON reading only)
    ```json
    {
      "CORSRules": [
        {
          "AllowedOrigins": ["https://frontend.example.com"],
          "AllowedMethods": ["GET"],
          "AllowedHeaders": ["*"],
          "ExposeHeaders": ["Content-Length", "Content-Type"]
        }
      ]
    }
    ```
    * **Use Case:** SPA reading JSON files for config/data
    * **Risk:** Safe if domain is controlled

<br>

* **Common CORS Errors & Why They Happen**
  * Error: ‚ÄúNo ‚ÄòAccess-Control-Allow-Origin‚Äô header present‚Äù
    * Origin not included in AllowedOrigins
  * Error: ‚ÄúMethod not allowed by Access-Control-Allow-Methods‚Äù
    * Missing PUT/POST in AllowedMethods
  * Error: ‚ÄúRequest header not allowed‚Äù
    * Custom header missing in AllowedHeaders
  * Error: Preflight request fails
    * OPTIONS method not allowed


---

### Presigned-Url
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html
* A **Presigned URL** is a special, time-limited URL that grants temporary access to an S3 object until the URL expires.
* It allows users to **download (GET)** or **upload (PUT)** files securely without requiring AWS credentials.
* URL is digitally signed using the bucket owner's AWS credentials.

* Perfect for:
  * Browser uploads
  * Secure file sharing
  * Temporary external access

* **Why Presigned URLs Are Used**
  * Secure temporary access to private S3 objects.
  * Allow users to upload files without exposing AWS keys on the frontend.
  * Let external parties download one specific file without making the whole bucket public.
  * Ensure access expires automatically after a specific time (1 minute, 15 minutes, 1 hour, etc.).

* **How Presigned URL Works (Internal Flow)**
  * Backend generates a signed URL using AWS SDK (Node.js, Python, Java, Go, etc.)
  * URL contains:
    * Bucket name
    * Object key
    * Expiration timestamp
    * Signature (HMAC using AWS secret key)
  * Client (browser or mobile app) uses the URL directly ‚Üí no AWS credentials required
  * AWS S3 validates:
    * Signature correctness
    * Expiration time
    * HTTP method allowed (GET/PUT)
  * If valid ‚Üí access is granted
  * Once expired ‚Üí URL stops working automatically

* **Presigned URL Types**
  * GET Presigned URL
    * Used to download/read an object
    * HTTP Method: `GET`
  * PUT Presigned URL
    * Used to upload/write an object
    * HTTP Method: `PUT`

* **Presigned URL Advantages**
  * NO need to expose AWS access keys
  * Temporary ‚Üí automatically expires
  * Access is scoped to **one object only**
  * Works with private buckets
  * No CORS issues if properly configured
  * No need to make bucket public
  * Can restrict:
    * method
    * expiration time
    * content-type


* **Presigned URL Limitations**
  * Limited security scope:
    * If someone gets the URL, they can use it until expiration
  * Cannot prevent multiple downloads during validity period
  * Expiration time is limited (max ~7 days depending on SDK)

* **Expiration Time Rules**
  * Default expiration often: **3600 seconds (1 hour)**
  * You can set your own time:
    * 5 minutes
    * 15 minutes
    * 1 hour
    * 12 hours
    * 7 days max

* **Generate a Pre-Signed URL Using AWS CLI**
  * For Download (GET)
    ```bash
    aws s3 presign s3://your-bucket-name/object-name --expires-in 3600
    ```
    ```bash
    aws s3 presign s3://mahinraza/testfile.txt --expires-in 600
    ```
    * This generates a URL valid for **10 minutes (600 seconds)**.
    * You can share this URL ‚Äî anyone can download that object without AWS credentials until it expires.
  
  * For Upload (PUT)
    * You can‚Äôt directly presign a PUT URL via `aws s3 presign`,
    * so use the AWS SDK (Python, Node.js, etc.) ‚Äî see below.

* **Backend Code Examples for Generating Presigned URLs**
  * Generate a Pre-Signed URL Using Python (Boto3) 
    ```python
    import boto3

    s3 = boto3.client('s3')

    url = s3.generate_presigned_url(
        'get_object',
        Params={'Bucket': 'mybucket', 'Key': 'file.txt'},
        ExpiresIn=3600
    )

    print("Download URL:", url)
    ```
    ```bash
    https://mahinraza.s3.amazonaws.com/testfile.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...
    ```

  * Pre-Signed URL for Upload (PUT)
    ```python
    import boto3

    s3 = boto3.client('s3')

    url = s3.generate_presigned_url(
        'put_object',
        Params={'Bucket': 'mahinraza', 'Key': 'README.md'},
        ExpiresIn=900  # 15 minutes
    )

    print("Upload URL:", url)
    ```
    ```bash
    curl -X PUT -T README.md "https://mahinraza.s3.amazonaws.com/README.md?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=..."
    ```

  * Node.js (AWS SDK v3)
    ```javascript
    import { S3Client, GetObjectCommand, PutObjectCommand } from "@aws-sdk/client-s3";
    import { getSignedUrl } from "@aws-sdk/s3-request-presigner";

    const client = new S3Client({ region: "ap-south-1" });

    const url = await getSignedUrl(
      client,
      new GetObjectCommand({
        Bucket: "mybucket",
        Key: "file.txt"
      }),
      { expiresIn: 3600 }
    );

    console.log(url);
    ```

<br>

| Parameter     | Description                      |
| ------------- | -------------------------------- |
| **Bucket**    | Your S3 bucket name              |
| **Key**       | Object name (file name/path)     |
| **ExpiresIn** | Validity in seconds (max 7 days) |
| **Operation** | `'get_object'` or `'put_object'` |
| **Params**    | Dictionary with object details   |

<br>

* **Security Tips**
  * Set **short expiration** (5‚Äì15 mins) for uploads.
  * Use **IAM policies** to restrict who can generate URLs.
  * Don‚Äôt expose your AWS keys publicly ‚Äî only share the pre-signed link.
  * Ensure your **bucket policy doesn‚Äôt block HTTPS** (since pre-signed URLs use HTTPS).

<br>

* **Summary**

| Purpose        | Command / Code                                     | Output                         |
| -------------- | -------------------------------------------------- | ------------------------------ |
| Download (GET) | `aws s3 presign s3://bucket/file --expires-in 600` | Temporary public download link |
| Upload (PUT)   | Python `generate_presigned_url('put_object', ‚Ä¶)`   | Temporary upload link          |
| Works With     | HTTPS only                                         | Safe & temporary access        |
| Max Duration   | 7 days                                             | Adjustable                     |


* **How Presigned URL Helps Upload from Browser**
  * Frontend requests URL from backend
  * Backend generates PUT presigned URL
  * Frontend uploads file directly to S3 using this URL
  * No need for backend to handle the file
  * Saves bandwidth + reduces load on backend server

* **Presigned URL + CORS**
  * When uploading from browser:
    * S3 bucket must have a CORS policy
    * Allow methods: PUT, POST, GET
    * Allow header: Content-Type (if needed)

* **Presigned POST vs Presigned URL**

  | Feature                   | Presigned GET | Presigned PUT | Presigned POST                      |
  | ------------------------- | ------------- | ------------- | ----------------------------------- |
  | Download                  | ‚úÖ Yes         | ‚ùå No          | ‚ùå No                                |
  | Upload                    | ‚ùå No          | ‚úÖ Yes         | ‚úÖ Yes                               |
  | Supports multipart upload | ‚ùå No          | ‚úÖ Yes         | ‚úÖ Yes (form data)                   |
  | Browser-native upload     | Medium        | Easy          | Easy                                |
  | Conditions support        | No            | No            | Yes (Content-Type, ACL, size limit) |

* **Common Real-Life Use Cases**
  * Allow users to upload profile photos to S3
  * Let customers download invoices securely
  * Temporary shareable link (e.g., send email with link, expires in 15 mins)
  * Mobile app downloading app updates
  * Customer-specific folders access

---


### Encryption at Rest (What it is + Why it matters)
  * "Encryption at rest" means your data is **encrypted on disk** when stored inside AWS S3.
  * Even if someone accesses the physical storage media, they can't read your data without the key.
  * AWS automatically protects data on the backend using cryptographic systems.
  * Protects against unauthorized access at storage layer
  * Meets compliance requirements (HIPAA, GDPR, PCI DSS)
  * AWS manages key rotation and encryption infrastructure
  * Set on both bucket level or on object(when uploading)
  * Encryption can be changed on the go but it required versioning enabled because aws create a new version of old object encrypted with new key.

* **Accessing S3 Objects ‚Äì HTTP vs HTTPS**\
* When you access an S3 object using its public URL, AWS automatically allows access over HTTPS (secure) and HTTP (insecure).
* HTTPS means:
  * Data travels in encrypted form
  * Connection is secure and trusted
  * Recommended by AWS
* HTTP means:
  * Data travels without encryption
  * Browser shows "Not Secure"
  * AWS allows it but does not recommend using it
* So, although both work, always use HTTPS to protect data in transit.



* **Types of Encryption at Rest in S3**

* **Server-Side-Encryption**
  * AWS provides **three main server-side encryption(SSE) methods** and one client-side method:
  * Server-side encryption = AWS performs the encryption FOR you.
  * AWS handles encrypt before writing to disk and decrypt when retrieving automatically.

  **a) SSE-S3 (AES-256) ‚Äì Amazon Managed Keys**
    * AWS handles everything
    * No key management required
    * Bucket policy can enforce `"x-amz-server-side-encryption": "AES256"`
    * Uses AES-256 algorithm
    * Easiest option
    * Zero setup
    *  Automatically enabled by default on all new buckets (2023 update)

  **b) SSE-KMS ‚Äì AWS Key Management Service (KMS Keys)**
    * Uses customer-managed keys or AWS-managed KMS keys
    * More control and auditing
    * Encryption keys managed by **AWS KMS**
    * Supports key rotation, access control, CloudTrail audit logs
    * Fine-grained permissions using IAM and KMS
    * Track key usage in CloudTrail
    * Enforce access via policies
    * Additional **KMS request charges**
    * S3 replication requires additional KMS permissions
    * Decryption may be throttled by KMS limits

  **c) SSE-C ‚Äì Customer-Provided Keys**
    * You provide the encryption key with every request
    * AWS never stores the key
    * AWS uses the provided key to encrypt/decrypt data
    * Keys are discarded after processing
    * Strict compliance requirements where keys must never leave customer's environment
    * Not supported in replication
    * More complex to manage
    * Needs HTTPS always

* **Client-Side Encryption (CSE)**
  * Client-side encryption = **YOU encrypt data before uploading** to S3.
  * Data uploaded is already encrypted; AWS never sees plaintext.
  * Encryption happens on the client machine or your application server
  * Your app encrypts using your own encryption library (AWS SDK or custom)
  * You manage your encryption keys
  * S3 stores ciphertext/encrypted data
  * Maximum control of security
  * AWS never sees plaintext data
  * Works with sensitive regulated data
  * Considerations:
    * You must handle key management (KMS or external system)
    * You must decrypt data manually on download
    * More complex implementation


* **End-to-End Encryption Layer Summary**

| Layer                  | Who encrypts? | Who manages keys? | Where is encryption done? | Suitable when?                             |
| ---------------------- | ------------- | ----------------- | ------------------------- | ------------------------------------------ |
| Encryption at Rest     | AWS           | AWS               | On S3 storage servers     | Default protection                         |
| SSE-S3                 | S3            | AWS               | On upload                 | Simple, automatic, no overhead             |
| SSE-KMS                | S3            | AWS KMS           | On upload                 | Compliance, auditing, fine-grained control |
| SSE-C                  | S3            | YOU               | On upload                 | When regulation prevents AWS storing keys  |
| Client-Side Encryption | YOUR App      | YOU               | Before upload             | Max security, sensitive data               |


* **Example Scenario Explained (from transcript)**
  * User uploads files to an S3 bucket
  * AWS encrypts them using SSE-S3 or SSE-KMS
  * Encryption happens automatically
  * If SSE-KMS is used:
    * S3 makes a KMS API call for each upload
    * High volume = more cost
    * KMS rate limits can cause throttling
  * Solution ‚Üí Enable S3 Bucket Key to reduce KMS calls and cost significantly

* **How Encryption Works (Deep Internal Flow)**
  * Upload Flow (Example: SSE-KMS)
    1. Client uploads object ‚Üí includes header `"x-amz-server-side-encryption: aws:kms"`.
    2. S3 sends request to KMS to generate/unwrap data key.
    3. KMS returns data key (encrypted key + plaintext data key).
    4. S3 encrypts data using plaintext data key.
    5. S3 stores:
        * Encrypted object
        * Encrypted data key
        * Metadata with KMS key ID
  * Download Flow
    1. Client requests object.
    2. S3 reads encrypted object + encrypted key.
    3. S3 sends encrypted key to KMS to decrypt.
    4. KMS returns decrypted key.
    5. S3 decrypts data.
    6. Returns plaintext object to client.


* **Encryption + Replication (Important)**
  * **SSE-S3** ‚Üí Works perfectly with SRR/CRR
  * **SSE-KMS** ‚Üí Requires permissions on both source & destination keys
  * **SSE-C** ‚Üí ‚ùå Cannot be replicated
  * Client-side encrypted objects replicate as ciphertext

* **How to Enforce Encryption with Bucket Policy**
Example: enforce SSE-S3:
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [{
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::mybucket/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    }]
  }
  ```
Example: enforce SSE-KMS:
  ```json
  {
    "Version": "2012-10-17",
    "Statement": [{
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::mybucket/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "aws:kms"
        }
      }
    }]
  }
  ```


* **When to Use Which Type of Encryption (Real-Life Scenarios)**

  * Use SSE-S3 when:
    * Uploading images, logs, backups
    * Cost-efficient
    * No special compliance requirements

  * Use SSE-KMS when:
    * Compliance required (PCI, HIPAA, SOC2)
    * Need fine-grained access control
    * Need auditing (CloudTrail logs)
    * Multi-account replication with encryption

  * Use SSE-C when:
    * Regulatory data cannot be decrypted by AWS
    * You manage your own encryption keys entirely

  * Use Client-Side Encryption when:
    * Zero trust model
    * Highly sensitive documents (financial, medical, legal)
    * Fully confidential data required

* **Force HTTPS Access Only**
* Open S3 Bucket Policy
  * Go to AWS Console ‚Üí S3
  * Select your bucket
  * Go to Permissions tab
  * Scroll to Bucket Policy
  * Click Edit

* This policy blocks all HTTP access for all objects in the bucket.
  ```json
    {
    "Version": "2012-10-17",
    "Id": "ForceHTTPS",
    "Statement": [
      {
        "Sid": "DenyHTTPRequests",
        "Effect": "Deny",
        "Principal": "*",
        "Action": "s3:GetObject",
        "Resource": "arn:aws:s3:::your-bucket-name/*",
        "Condition": {
          "Bool": {
            "aws:SecureTransport": "false"
          }
        }
      }
    ]
  }
  ```
  * `aws:SecureTransport = false` ‚Üí The policy denies the request came over HTTP.
  * Browser shows:
    * ‚ÄúAccess Denied‚Äù
    * OR ‚Äú403 Forbidden‚Äù

  * Presigned URLs automatically include HTTPS by default, If someone tries to convert them to HTTP, policy blocks it.
  * To deny listing bucket over HTTP:
    ```json
    {
      "Sid": "DenyListOnHTTP",
      "Effect": "Deny",
      "Principal": "*",
      "Action": [
        "s3:ListBucket"
      ],
      "Resource": "arn:aws:s3:::your-bucket-name",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false"
        }
      }
    }
    ```