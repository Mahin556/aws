* **Export DynamoDB Data to S3 (Native Feature)**
  * Used to export **table data directly to an S3 bucket**
  * Fully managed by AWS
  * No impact on table performance
  * Works without Scan or custom scripts

* **What gets exported**
  * All items in the table
  * Attribute names and values
  * Data is exported as a **snapshot**
  * Export does **not affect live reads/writes**

* **Export source**
  * You can export from:
    * **Current table state**
    * **Point-in-Time Recovery (PITR)** timestamp (within last 35 days)

* **Export formats**
  * **DynamoDB JSON**
  * **ION**
  * **CSV** (limited / analytics use cases)
  * Commonly used for:
    * Athena
    * Glue
    * EMR
    * Redshift Spectrum

* **S3 requirements**
  * Bucket must be in the **same AWS Region**
  * Proper IAM permissions required:
    * DynamoDB → write to S3
  * AWS creates a folder-like prefix automatically

* **What is NOT exported**
  * Table metadata
  * Indexes (LSI / GSI)
  * Capacity settings
  * IAM policies

* **Consistency**
  * Export is **consistent to the selected timestamp**
  * Not affected by ongoing writes

* **Cost**
  * Charged based on:
    * Size of exported data
    * Export request
  * Separate from DynamoDB read/write cost
  * No RCU/WCU consumption

* **Typical use cases**
  * Data analytics
  * Data lake ingestion
  * Long-term archival
  * Compliance & audit
  * Backup for external systems

* **High-level steps (Console)**
  * Open DynamoDB
  * Select table
  * Choose **Export to S3**
  * Select:
    * Export source (current / PITR)
    * S3 bucket
    * Export format
  * Start export

* **Important limits**
  * Export is **table-level only**
  * Cannot export specific items
  * Export time depends on table size

* **Export vs Scan**
  * Export:
    * Fast
    * No throttling
    * No capacity usage
  * Scan:
    * Slow
    * Consumes RCUs
    * Expensive for large tables

* **Interview one-liners**
  * “DynamoDB export to S3 creates a consistent snapshot without using RCUs.”
  * “It supports exporting from PITR timestamps.”
  * “Best way to move DynamoDB data into analytics systems.”

---
---

* **Import Data into DynamoDB from S3**
  * Native AWS feature to **load large datasets from S3 into DynamoDB**
  * Fully managed, **no Scan, no scripts, no RCU/WCU usage**
  * Best for **bulk data loading**

* **Supported data formats**
  * **DynamoDB JSON**
  * **ION**
  * **CSV**
  * Data must already exist in an **S3 bucket**

* **How import works**
  * AWS reads data from S3
  * Creates a **new DynamoDB table**
  * Loads items in parallel
  * Import is **offline** (table not usable until import finishes)

* **Important rule**
  * ❌ You **cannot import into an existing table**
  * ✅ Import always creates a **new table**

* **Table creation during import**
  * You define:
    * Partition Key
    * Sort Key (optional)
    * Billing mode (On-Demand / Provisioned)
    * Encryption
  * Data in S3 must match key schema

* **Indexes**
  * You can define **GSIs during import**
  * LSIs ❌ not supported during import
  * Indexes are built after data load

* **What is NOT imported**
  * IAM policies
  * Auto Scaling settings
  * CloudWatch alarms
  * PITR (can enable later)

* **Consistency**
  * Import creates a **consistent dataset**
  * No partial writes
  * No impact on other tables

* **Cost**
  * Charged based on:
    * Data size imported
    * Import processing
  * No read/write capacity charges

* **S3 requirements**
  * Bucket must be in the **same AWS Region**
  * DynamoDB service role needs:
    * `s3:GetObject`
    * `s3:ListBucket`

* **Typical use cases**
  * Migrating data from:
    * S3 data lake
    * Old databases
    * Logs / analytics output
  * Initial bulk load
  * Rebuilding tables after schema redesign

* **High-level steps (Console)**
  * Open DynamoDB
  * Choose **Import from S3**
  * Select S3 bucket & prefix
  * Choose data format
  * Define table schema
  * Start import

* **Import vs BatchWriteItem**
  * Import:
    * Massive scale
    * Faster
    * No throttling
  * BatchWriteItem:
    * Limited (25 items/request)
    * Uses WCUs
    * App-managed

* **Interview one-liners**
  * “DynamoDB import from S3 creates a new table.”
  * “It does not consume RCUs or WCUs.”
  * “Best for bulk data ingestion into DynamoDB.”


---
---

* **S3 vs DynamoDB (Cost Perspective)**
  * **Amazon S3**
    * Designed for **cheap, large-scale storage**
    * Stores **massive amounts of data** (TBs / PBs)
    * Multiple low-cost storage classes:
      * Standard
      * Infrequent Access
      * Glacier / Deep Archive
    * Very **cost-effective for long-term storage**
    * Not optimized for millisecond queries
  * **DynamoDB**
    * Designed for **high-speed, low-latency access**
    * Optimized for **frequent reads and writes**
    * Cost includes:
      * Storage
      * Read/write capacity
      * Indexes
    * Much **more expensive** for storing large, rarely accessed data

* **Key Rule (Very Important)**
  * DynamoDB = **Database (fast access)**
  * S3 = **Data lake / storage (cheap & scalable)**
  * ❌ DynamoDB is NOT a replacement for S3
  * ❌ S3 is NOT a replacement for DynamoDB

* **Why exporting DynamoDB data to S3 makes sense**
  * Cost savings for:
    * Old / historical data
    * Infrequently accessed data
  * Long-term retention (**beyond PITR’s 35 days**)
  * Analytics use cases
  * Compliance & auditing

* **Export to S3 – What it enables**
  * Perform **ETL jobs** (Glue, Spark, EMR)
  * Query data using **Athena**
  * Integrate with other services or external applications
  * Archive DynamoDB data cheaply
  * Later **re-import data back into DynamoDB** if needed

* **Common real-world scenarios (from comments)**
  * Customer shares **huge CSV / Excel files**
    * Upload to S3
    * Import into DynamoDB for application usage
  * Analytics workloads
    * DynamoDB → Export to S3 → Analyze
  * Cross-account migration
    * Export from Account A → S3 → Import into Account B
  * Backup retention
    * Store DynamoDB data **longer than 35 days** (PITR limit)

* **Why not store everything directly in DynamoDB**
  * Very high storage cost
  * Index costs increase rapidly
  * Not designed for:
    * Full-table analytics
    * Long-term archival
    * Large file / CSV storage

* **Best-practice architecture**
  * DynamoDB:
    * Hot data
    * Frequently accessed
    * Application-facing
  * S3:
    * Cold data
    * Historical data
    * Analytics & backups

* **Interview-ready one-liners**
  * “S3 is cheaper than DynamoDB for large-scale storage.”
  * “DynamoDB is optimized for fast access, not cheap storage.”
  * “Exporting DynamoDB data to S3 enables analytics and long-term retention.”
  * “S3 helps retain DynamoDB backups beyond PITR’s 35-day limit.”

* **Final conclusion**
  * ✅ Yes, storing large amounts of data in S3 is **far more cost-effective**
  * ✅ DynamoDB should store **only the data you need fast**
  * ✅ S3 + DynamoDB together is the **recommended AWS design**

